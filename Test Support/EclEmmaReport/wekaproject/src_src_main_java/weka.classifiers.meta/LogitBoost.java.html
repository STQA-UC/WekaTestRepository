<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><meta http-equiv="Content-Type" content="text/html;charset=UTF-8"/><link rel="stylesheet" href="../../../.resources/report.css" type="text/css"/><link rel="shortcut icon" href="../../../.resources/report.gif" type="image/gif"/><title>LogitBoost.java</title><link rel="stylesheet" href="../../../.resources/prettify.css" type="text/css"/><script type="text/javascript" src="../../../.resources/prettify.js"></script></head><body onload="window['PR_TAB_WIDTH']=4;prettyPrint()"><div class="breadcrumb" id="breadcrumb"><span class="info"><a href="../../../.sessions.html" class="el_session">Sessions</a></span><a href="../../../index.html" class="el_report">AllTests (Nov 28, 2015 2:34:31 PM)</a> &gt; <a href="../../index.html" class="el_group">wekaproject</a> &gt; <a href="../index.html" class="el_bundle">src/src/main/java</a> &gt; <a href="index.source.html" class="el_package">weka.classifiers.meta</a> &gt; <span class="el_source">LogitBoost.java</span></div><h1>LogitBoost.java</h1><pre class="source lang-java linenums">/*
 *    This program is free software; you can redistribute it and/or modify
 *    it under the terms of the GNU General Public License as published by
 *    the Free Software Foundation; either version 2 of the License, or
 *    (at your option) any later version.
 *
 *    This program is distributed in the hope that it will be useful,
 *    but WITHOUT ANY WARRANTY; without even the implied warranty of
 *    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 *    GNU General Public License for more details.
 *
 *    You should have received a copy of the GNU General Public License
 *    along with this program; if not, write to the Free Software
 *    Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
 */

/*
 *    LogitBoost.java
 *    Copyright (C) 1999, 2002 University of Waikato, Hamilton, New Zealand
 *
 */

package weka.classifiers.meta;

import weka.classifiers.Classifier;
import weka.classifiers.Evaluation;
import weka.classifiers.RandomizableIteratedSingleClassifierEnhancer;
import weka.classifiers.Sourcable;
import weka.core.Attribute;
import weka.core.Capabilities;
import weka.core.Instance;
import weka.core.Instances;
import weka.core.Option;
import weka.core.RevisionUtils;
import weka.core.TechnicalInformation;
import weka.core.TechnicalInformationHandler;
import weka.core.Utils;
import weka.core.WeightedInstancesHandler;
import weka.core.Capabilities.Capability;
import weka.core.TechnicalInformation.Field;
import weka.core.TechnicalInformation.Type;

import java.util.Enumeration;
import java.util.Random;
import java.util.Vector;

/**
 &lt;!-- globalinfo-start --&gt;
 * Class for performing additive logistic regression. &lt;br/&gt;
 * This class performs classification using a regression scheme as the base learner, and can handle multi-class problems.  For more information, see&lt;br/&gt;
 * &lt;br/&gt;
 * J. Friedman, T. Hastie, R. Tibshirani (1998). Additive Logistic Regression: a Statistical View of Boosting. Stanford University.&lt;br/&gt;
 * &lt;br/&gt;
 * Can do efficient internal cross-validation to determine appropriate number of iterations.
 * &lt;p/&gt;
 &lt;!-- globalinfo-end --&gt;
 *
 &lt;!-- technical-bibtex-start --&gt;
 * BibTeX:
 * &lt;pre&gt;
 * &amp;#64;techreport{Friedman1998,
 *    address = {Stanford University},
 *    author = {J. Friedman and T. Hastie and R. Tibshirani},
 *    title = {Additive Logistic Regression: a Statistical View of Boosting},
 *    year = {1998},
 *    PS = {http://www-stat.stanford.edu/\~jhf/ftp/boost.ps}
 * }
 * &lt;/pre&gt;
 * &lt;p/&gt;
 &lt;!-- technical-bibtex-end --&gt;
 *
 &lt;!-- options-start --&gt;
 * Valid options are: &lt;p/&gt;
 * 
 * &lt;pre&gt; -Q
 *  Use resampling instead of reweighting for boosting.&lt;/pre&gt;
 * 
 * &lt;pre&gt; -P &amp;lt;percent&amp;gt;
 *  Percentage of weight mass to base training on.
 *  (default 100, reduce to around 90 speed up)&lt;/pre&gt;
 * 
 * &lt;pre&gt; -F &amp;lt;num&amp;gt;
 *  Number of folds for internal cross-validation.
 *  (default 0 -- no cross-validation)&lt;/pre&gt;
 * 
 * &lt;pre&gt; -R &amp;lt;num&amp;gt;
 *  Number of runs for internal cross-validation.
 *  (default 1)&lt;/pre&gt;
 * 
 * &lt;pre&gt; -L &amp;lt;num&amp;gt;
 *  Threshold on the improvement of the likelihood.
 *  (default -Double.MAX_VALUE)&lt;/pre&gt;
 * 
 * &lt;pre&gt; -H &amp;lt;num&amp;gt;
 *  Shrinkage parameter.
 *  (default 1)&lt;/pre&gt;
 * 
 * &lt;pre&gt; -S &amp;lt;num&amp;gt;
 *  Random number seed.
 *  (default 1)&lt;/pre&gt;
 * 
 * &lt;pre&gt; -I &amp;lt;num&amp;gt;
 *  Number of iterations.
 *  (default 10)&lt;/pre&gt;
 * 
 * &lt;pre&gt; -D
 *  If set, classifier is run in debug mode and
 *  may output additional info to the console&lt;/pre&gt;
 * 
 * &lt;pre&gt; -W
 *  Full name of base classifier.
 *  (default: weka.classifiers.trees.DecisionStump)&lt;/pre&gt;
 * 
 * &lt;pre&gt; 
 * Options specific to classifier weka.classifiers.trees.DecisionStump:
 * &lt;/pre&gt;
 * 
 * &lt;pre&gt; -D
 *  If set, classifier is run in debug mode and
 *  may output additional info to the console&lt;/pre&gt;
 * 
 &lt;!-- options-end --&gt;
 *
 * Options after -- are passed to the designated learner.&lt;p&gt;
 *
 * @author Len Trigg (trigg@cs.waikato.ac.nz)
 * @author Eibe Frank (eibe@cs.waikato.ac.nz)
 * @version $Revision: 9371 $ 
 */
public class LogitBoost 
  extends RandomizableIteratedSingleClassifierEnhancer
  implements Sourcable, WeightedInstancesHandler, TechnicalInformationHandler {

  /** for serialization */
  private static final long serialVersionUID = 8627452775249625582L;

  /** Array for storing the generated base classifiers. 
   Note: we are hiding the variable from IteratedSingleClassifierEnhancer*/
  protected Classifier [][] m_Classifiers;

  /** The number of classes */
  protected int m_NumClasses;

  /** The number of successfully generated base classifiers. */
  protected int m_NumGenerated;

  /** The number of folds for the internal cross-validation. */
<span class="fc" id="L148">  protected int m_NumFolds = 0;</span>

  /** The number of runs for the internal cross-validation. */
<span class="fc" id="L151">  protected int m_NumRuns = 1;</span>

  /** Weight thresholding. The percentage of weight mass used in training */
<span class="fc" id="L154">  protected int m_WeightThreshold = 100;</span>

  /** A threshold for responses (Friedman suggests between 2 and 4) */
  protected static final double Z_MAX = 3;

  /** Dummy dataset with a numeric class */
  protected Instances m_NumericClassData;

  /** The actual class attribute (for getting class names) */
  protected Attribute m_ClassAttribute;

  /** Use boosting with reweighting? */
  protected boolean m_UseResampling;

  /** The threshold on the improvement of the likelihood */   
<span class="fc" id="L169">  protected double m_Precision = -Double.MAX_VALUE;</span>

  /** The value of the shrinkage parameter */
<span class="fc" id="L172">  protected double m_Shrinkage = 1;</span>

  /** The random number generator used */
<span class="fc" id="L175">  protected Random m_RandomInstance = null;</span>

  /** The value by which the actual target value for the
      true class is offset. */
<span class="fc" id="L179">  protected double m_Offset = 0.0;</span>
    
  /** a ZeroR model in case no model can be built from the data */
  protected Classifier m_ZeroR;
    
  /**
   * Returns a string describing classifier
   * @return a description suitable for
   * displaying in the explorer/experimenter gui
   */
  public String globalInfo() {

<span class="nc" id="L191">    return &quot;Class for performing additive logistic regression. \n&quot;</span>
      + &quot;This class performs classification using a regression scheme as the &quot;
      + &quot;base learner, and can handle multi-class problems.  For more &quot;
      + &quot;information, see\n\n&quot;
<span class="nc" id="L195">      + getTechnicalInformation().toString() + &quot;\n\n&quot;</span>
<span class="nc" id="L196">      + &quot;Can do efficient internal cross-validation to determine &quot;</span>
<span class="nc" id="L197">      + &quot;appropriate number of iterations.&quot;;</span>
  }
    
  /**
   * Constructor.
   */
<span class="fc" id="L203">  public LogitBoost() {</span>
    
<span class="fc" id="L205">    m_Classifier = new weka.classifiers.trees.DecisionStump();</span>
<span class="fc" id="L206">  }</span>

  /**
   * Returns an instance of a TechnicalInformation object, containing 
   * detailed information about the technical background of this class,
   * e.g., paper reference or book this class is based on.
   * 
   * @return the technical information about this class
   */
  public TechnicalInformation getTechnicalInformation() {
    TechnicalInformation 	result;
    
<span class="nc" id="L218">    result = new TechnicalInformation(Type.TECHREPORT);</span>
<span class="nc" id="L219">    result.setValue(Field.AUTHOR, &quot;J. Friedman and T. Hastie and R. Tibshirani&quot;);</span>
<span class="nc" id="L220">    result.setValue(Field.YEAR, &quot;1998&quot;);</span>
<span class="nc" id="L221">    result.setValue(Field.TITLE, &quot;Additive Logistic Regression: a Statistical View of Boosting&quot;);</span>
<span class="nc" id="L222">    result.setValue(Field.ADDRESS, &quot;Stanford University&quot;);</span>
<span class="nc" id="L223">    result.setValue(Field.PS, &quot;http://www-stat.stanford.edu/~jhf/ftp/boost.ps&quot;);</span>
    
<span class="nc" id="L225">    return result;</span>
  }

  /**
   * String describing default classifier.
   * 
   * @return the default classifier classname
   */
  protected String defaultClassifierString() {
    
<span class="fc" id="L235">    return &quot;weka.classifiers.trees.DecisionStump&quot;;</span>
  }

  /**
   * Select only instances with weights that contribute to 
   * the specified quantile of the weight distribution
   *
   * @param data the input instances
   * @param quantile the specified quantile eg 0.9 to select 
   * 90% of the weight mass
   * @return the selected instances
   */
  protected Instances selectWeightQuantile(Instances data, double quantile) { 

<span class="nc" id="L249">    int numInstances = data.numInstances();</span>
<span class="nc" id="L250">    Instances trainData = new Instances(data, numInstances);</span>
<span class="nc" id="L251">    double [] weights = new double [numInstances];</span>

<span class="nc" id="L253">    double sumOfWeights = 0;</span>
<span class="nc bnc" id="L254" title="All 2 branches missed.">    for (int i = 0; i &lt; numInstances; i++) {</span>
<span class="nc" id="L255">      weights[i] = data.instance(i).weight();</span>
<span class="nc" id="L256">      sumOfWeights += weights[i];</span>
    }
<span class="nc" id="L258">    double weightMassToSelect = sumOfWeights * quantile;</span>
<span class="nc" id="L259">    int [] sortedIndices = Utils.sort(weights);</span>

    // Select the instances
<span class="nc" id="L262">    sumOfWeights = 0;</span>
<span class="nc bnc" id="L263" title="All 2 branches missed.">    for (int i = numInstances-1; i &gt;= 0; i--) {</span>
<span class="nc" id="L264">      Instance instance = (Instance)data.instance(sortedIndices[i]).copy();</span>
<span class="nc" id="L265">      trainData.add(instance);</span>
<span class="nc" id="L266">      sumOfWeights += weights[sortedIndices[i]];</span>
<span class="nc bnc" id="L267" title="All 2 branches missed.">      if ((sumOfWeights &gt; weightMassToSelect) &amp;&amp; </span>
<span class="nc bnc" id="L268" title="All 2 branches missed.">	  (i &gt; 0) &amp;&amp; </span>
<span class="nc bnc" id="L269" title="All 2 branches missed.">	  (weights[sortedIndices[i]] != weights[sortedIndices[i-1]])) {</span>
<span class="nc" id="L270">	break;</span>
      }
    }
<span class="nc bnc" id="L273" title="All 2 branches missed.">    if (m_Debug) {</span>
<span class="nc" id="L274">      System.err.println(&quot;Selected &quot; + trainData.numInstances()</span>
<span class="nc" id="L275">			 + &quot; out of &quot; + numInstances);</span>
    }
<span class="nc" id="L277">    return trainData;</span>
  }

  /**
   * Returns an enumeration describing the available options.
   *
   * @return an enumeration of all the available options.
   */
  public Enumeration listOptions() {

<span class="fc" id="L287">    Vector newVector = new Vector(6);</span>

<span class="fc" id="L289">    newVector.addElement(new Option(</span>
<span class="fc" id="L290">	      &quot;\tUse resampling instead of reweighting for boosting.&quot;,</span>
<span class="fc" id="L291">	      &quot;Q&quot;, 0, &quot;-Q&quot;));</span>
<span class="fc" id="L292">    newVector.addElement(new Option(</span>
<span class="fc" id="L293">	      &quot;\tPercentage of weight mass to base training on.\n&quot;</span>
	      +&quot;\t(default 100, reduce to around 90 speed up)&quot;,
<span class="fc" id="L295">	      &quot;P&quot;, 1, &quot;-P &lt;percent&gt;&quot;));</span>
<span class="fc" id="L296">    newVector.addElement(new Option(</span>
<span class="fc" id="L297">	      &quot;\tNumber of folds for internal cross-validation.\n&quot;</span>
	      +&quot;\t(default 0 -- no cross-validation)&quot;,
<span class="fc" id="L299">	      &quot;F&quot;, 1, &quot;-F &lt;num&gt;&quot;));</span>
<span class="fc" id="L300">    newVector.addElement(new Option(</span>
<span class="fc" id="L301">	      &quot;\tNumber of runs for internal cross-validation.\n&quot;</span>
	      +&quot;\t(default 1)&quot;,
<span class="fc" id="L303">	      &quot;R&quot;, 1, &quot;-R &lt;num&gt;&quot;));</span>
<span class="fc" id="L304">    newVector.addElement(new Option(</span>
<span class="fc" id="L305">	      &quot;\tThreshold on the improvement of the likelihood.\n&quot;</span>
	      +&quot;\t(default -Double.MAX_VALUE)&quot;,
<span class="fc" id="L307">	      &quot;L&quot;, 1, &quot;-L &lt;num&gt;&quot;));</span>
<span class="fc" id="L308">    newVector.addElement(new Option(</span>
<span class="fc" id="L309">	      &quot;\tShrinkage parameter.\n&quot;</span>
	      +&quot;\t(default 1)&quot;,
<span class="fc" id="L311">	      &quot;H&quot;, 1, &quot;-H &lt;num&gt;&quot;));</span>

<span class="fc" id="L313">    Enumeration enu = super.listOptions();</span>
<span class="fc bfc" id="L314" title="All 2 branches covered.">    while (enu.hasMoreElements()) {</span>
<span class="fc" id="L315">      newVector.addElement(enu.nextElement());</span>
    }
<span class="fc" id="L317">    return newVector.elements();</span>
  }


  /**
   * Parses a given list of options. &lt;p/&gt;
   * 
   &lt;!-- options-start --&gt;
   * Valid options are: &lt;p/&gt;
   * 
   * &lt;pre&gt; -Q
   *  Use resampling instead of reweighting for boosting.&lt;/pre&gt;
   * 
   * &lt;pre&gt; -P &amp;lt;percent&amp;gt;
   *  Percentage of weight mass to base training on.
   *  (default 100, reduce to around 90 speed up)&lt;/pre&gt;
   * 
   * &lt;pre&gt; -F &amp;lt;num&amp;gt;
   *  Number of folds for internal cross-validation.
   *  (default 0 -- no cross-validation)&lt;/pre&gt;
   * 
   * &lt;pre&gt; -R &amp;lt;num&amp;gt;
   *  Number of runs for internal cross-validation.
   *  (default 1)&lt;/pre&gt;
   * 
   * &lt;pre&gt; -L &amp;lt;num&amp;gt;
   *  Threshold on the improvement of the likelihood.
   *  (default -Double.MAX_VALUE)&lt;/pre&gt;
   * 
   * &lt;pre&gt; -H &amp;lt;num&amp;gt;
   *  Shrinkage parameter.
   *  (default 1)&lt;/pre&gt;
   * 
   * &lt;pre&gt; -S &amp;lt;num&amp;gt;
   *  Random number seed.
   *  (default 1)&lt;/pre&gt;
   * 
   * &lt;pre&gt; -I &amp;lt;num&amp;gt;
   *  Number of iterations.
   *  (default 10)&lt;/pre&gt;
   * 
   * &lt;pre&gt; -D
   *  If set, classifier is run in debug mode and
   *  may output additional info to the console&lt;/pre&gt;
   * 
   * &lt;pre&gt; -W
   *  Full name of base classifier.
   *  (default: weka.classifiers.trees.DecisionStump)&lt;/pre&gt;
   * 
   * &lt;pre&gt; 
   * Options specific to classifier weka.classifiers.trees.DecisionStump:
   * &lt;/pre&gt;
   * 
   * &lt;pre&gt; -D
   *  If set, classifier is run in debug mode and
   *  may output additional info to the console&lt;/pre&gt;
   * 
   &lt;!-- options-end --&gt;
   *
   * Options after -- are passed to the designated learner.&lt;p&gt;
   *
   * @param options the list of options as an array of strings
   * @throws Exception if an option is not supported
   */
  public void setOptions(String[] options) throws Exception {
    
<span class="fc" id="L383">    String numFolds = Utils.getOption('F', options);</span>
<span class="fc bfc" id="L384" title="All 2 branches covered.">    if (numFolds.length() != 0) {</span>
<span class="fc" id="L385">      setNumFolds(Integer.parseInt(numFolds));</span>
    } else {
<span class="fc" id="L387">      setNumFolds(0);</span>
    }
    
<span class="fc" id="L390">    String numRuns = Utils.getOption('R', options);</span>
<span class="fc bfc" id="L391" title="All 2 branches covered.">    if (numRuns.length() != 0) {</span>
<span class="fc" id="L392">      setNumRuns(Integer.parseInt(numRuns));</span>
    } else {
<span class="fc" id="L394">      setNumRuns(1);</span>
    }

<span class="fc" id="L397">    String thresholdString = Utils.getOption('P', options);</span>
<span class="fc bfc" id="L398" title="All 2 branches covered.">    if (thresholdString.length() != 0) {</span>
<span class="fc" id="L399">      setWeightThreshold(Integer.parseInt(thresholdString));</span>
    } else {
<span class="fc" id="L401">      setWeightThreshold(100);</span>
    }

<span class="fc" id="L404">    String precisionString = Utils.getOption('L', options);</span>
<span class="fc bfc" id="L405" title="All 2 branches covered.">    if (precisionString.length() != 0) {</span>
<span class="fc" id="L406">      setLikelihoodThreshold(new Double(precisionString).</span>
<span class="fc" id="L407">	doubleValue());</span>
    } else {
<span class="fc" id="L409">      setLikelihoodThreshold(-Double.MAX_VALUE);</span>
    }

<span class="fc" id="L412">    String shrinkageString = Utils.getOption('H', options);</span>
<span class="fc bfc" id="L413" title="All 2 branches covered.">    if (shrinkageString.length() != 0) {</span>
<span class="fc" id="L414">      setShrinkage(new Double(shrinkageString).</span>
<span class="fc" id="L415">	doubleValue());</span>
    } else {
<span class="fc" id="L417">      setShrinkage(1.0);</span>
    }

<span class="fc" id="L420">    setUseResampling(Utils.getFlag('Q', options));</span>
<span class="pc bpc" id="L421" title="3 of 4 branches missed.">    if (m_UseResampling &amp;&amp; (thresholdString.length() != 0)) {</span>
<span class="nc" id="L422">      throw new Exception(&quot;Weight pruning with resampling&quot;+</span>
			  &quot;not allowed.&quot;);
    }

<span class="fc" id="L426">    super.setOptions(options);</span>
<span class="fc" id="L427">  }</span>

  /**
   * Gets the current settings of the Classifier.
   *
   * @return an array of strings suitable for passing to setOptions
   */
  public String [] getOptions() {

<span class="fc" id="L436">    String [] superOptions = super.getOptions();</span>
<span class="fc" id="L437">    String [] options = new String [superOptions.length + 10];</span>

<span class="fc" id="L439">    int current = 0;</span>
<span class="pc bpc" id="L440" title="1 of 2 branches missed.">    if (getUseResampling()) {</span>
<span class="nc" id="L441">      options[current++] = &quot;-Q&quot;;</span>
    } else {
<span class="fc" id="L443">      options[current++] = &quot;-P&quot;; </span>
<span class="fc" id="L444">      options[current++] = &quot;&quot; + getWeightThreshold();</span>
    }
<span class="fc" id="L446">    options[current++] = &quot;-F&quot;; options[current++] = &quot;&quot; + getNumFolds();</span>
<span class="fc" id="L447">    options[current++] = &quot;-R&quot;; options[current++] = &quot;&quot; + getNumRuns();</span>
<span class="fc" id="L448">    options[current++] = &quot;-L&quot;; options[current++] = &quot;&quot; + getLikelihoodThreshold();</span>
<span class="fc" id="L449">    options[current++] = &quot;-H&quot;; options[current++] = &quot;&quot; + getShrinkage();</span>

<span class="fc" id="L451">    System.arraycopy(superOptions, 0, options, current, </span>
<span class="fc" id="L452">		     superOptions.length);</span>
<span class="fc" id="L453">    current += superOptions.length;</span>
<span class="pc bpc" id="L454" title="1 of 2 branches missed.">    while (current &lt; options.length) {</span>
<span class="nc" id="L455">      options[current++] = &quot;&quot;;</span>
    }
<span class="fc" id="L457">    return options;</span>
  }
  
  /**
   * Returns the tip text for this property
   * @return tip text for this property suitable for
   * displaying in the explorer/experimenter gui
   */
  public String shrinkageTipText() {
<span class="nc" id="L466">    return &quot;Shrinkage parameter (use small value like 0.1 to reduce &quot;</span>
      + &quot;overfitting).&quot;;
  }
			 
  /**
   * Get the value of Shrinkage.
   *
   * @return Value of Shrinkage.
   */
  public double getShrinkage() {
    
<span class="fc" id="L477">    return m_Shrinkage;</span>
  }
  
  /**
   * Set the value of Shrinkage.
   *
   * @param newShrinkage Value to assign to Shrinkage.
   */
  public void setShrinkage(double newShrinkage) {
    
<span class="fc" id="L487">    m_Shrinkage = newShrinkage;</span>
<span class="fc" id="L488">  }</span>
  
  /**
   * Returns the tip text for this property
   * @return tip text for this property suitable for
   * displaying in the explorer/experimenter gui
   */
  public String likelihoodThresholdTipText() {
<span class="nc" id="L496">    return &quot;Threshold on improvement in likelihood.&quot;;</span>
  }
			 
  /**
   * Get the value of Precision.
   *
   * @return Value of Precision.
   */
  public double getLikelihoodThreshold() {
    
<span class="fc" id="L506">    return m_Precision;</span>
  }
  
  /**
   * Set the value of Precision.
   *
   * @param newPrecision Value to assign to Precision.
   */
  public void setLikelihoodThreshold(double newPrecision) {
    
<span class="fc" id="L516">    m_Precision = newPrecision;</span>
<span class="fc" id="L517">  }</span>
  
  /**
   * Returns the tip text for this property
   * @return tip text for this property suitable for
   * displaying in the explorer/experimenter gui
   */
  public String numRunsTipText() {
<span class="nc" id="L525">    return &quot;Number of runs for internal cross-validation.&quot;;</span>
  }
  
  /**
   * Get the value of NumRuns.
   *
   * @return Value of NumRuns.
   */
  public int getNumRuns() {
    
<span class="fc" id="L535">    return m_NumRuns;</span>
  }
  
  /**
   * Set the value of NumRuns.
   *
   * @param newNumRuns Value to assign to NumRuns.
   */
  public void setNumRuns(int newNumRuns) {
    
<span class="fc" id="L545">    m_NumRuns = newNumRuns;</span>
<span class="fc" id="L546">  }</span>
  
  /**
   * Returns the tip text for this property
   * @return tip text for this property suitable for
   * displaying in the explorer/experimenter gui
   */
  public String numFoldsTipText() {
<span class="nc" id="L554">    return &quot;Number of folds for internal cross-validation (default 0 &quot;</span>
      + &quot;means no cross-validation is performed).&quot;;
  }
  
  /**
   * Get the value of NumFolds.
   *
   * @return Value of NumFolds.
   */
  public int getNumFolds() {
    
<span class="fc" id="L565">    return m_NumFolds;</span>
  }
  
  /**
   * Set the value of NumFolds.
   *
   * @param newNumFolds Value to assign to NumFolds.
   */
  public void setNumFolds(int newNumFolds) {
    
<span class="fc" id="L575">    m_NumFolds = newNumFolds;</span>
<span class="fc" id="L576">  }</span>
  
  /**
   * Returns the tip text for this property
   * @return tip text for this property suitable for
   * displaying in the explorer/experimenter gui
   */
  public String useResamplingTipText() {
<span class="nc" id="L584">    return &quot;Whether resampling is used instead of reweighting.&quot;;</span>
  }
  
  /**
   * Set resampling mode
   *
   * @param r true if resampling should be done
   */
  public void setUseResampling(boolean r) {
    
<span class="fc" id="L594">    m_UseResampling = r;</span>
<span class="fc" id="L595">  }</span>

  /**
   * Get whether resampling is turned on
   *
   * @return true if resampling output is on
   */
  public boolean getUseResampling() {
    
<span class="fc" id="L604">    return m_UseResampling;</span>
  }
  
  /**
   * Returns the tip text for this property
   * @return tip text for this property suitable for
   * displaying in the explorer/experimenter gui
   */
  public String weightThresholdTipText() {
<span class="nc" id="L613">    return &quot;Weight threshold for weight pruning (reduce to 90 &quot;</span>
      + &quot;for speeding up learning process).&quot;;
  }

  /**
   * Set weight thresholding
   *
   * @param threshold the percentage of weight mass used for training
   */
  public void setWeightThreshold(int threshold) {

<span class="fc" id="L624">    m_WeightThreshold = threshold;</span>
<span class="fc" id="L625">  }</span>

  /**
   * Get the degree of weight thresholding
   *
   * @return the percentage of weight mass used for training
   */
  public int getWeightThreshold() {

<span class="fc" id="L634">    return m_WeightThreshold;</span>
  }

  /**
   * Returns default capabilities of the classifier.
   *
   * @return      the capabilities of this classifier
   */
  public Capabilities getCapabilities() {
<span class="fc" id="L643">    Capabilities result = super.getCapabilities();</span>

    // class
<span class="fc" id="L646">    result.disableAllClasses();</span>
<span class="fc" id="L647">    result.disableAllClassDependencies();</span>
<span class="fc" id="L648">    result.enable(Capability.NOMINAL_CLASS);</span>
    
<span class="fc" id="L650">    return result;</span>
  }

  /**
   * Builds the boosted classifier
   * 
   * @param data the data to train the classifier with
   * @throws Exception if building fails, e.g., can't handle data
   */
  public void buildClassifier(Instances data) throws Exception {

<span class="fc" id="L661">    m_RandomInstance = new Random(m_Seed);</span>
<span class="fc" id="L662">    int classIndex = data.classIndex();</span>

<span class="pc bpc" id="L664" title="1 of 2 branches missed.">    if (m_Classifier == null) {</span>
<span class="nc" id="L665">      throw new Exception(&quot;A base classifier has not been specified!&quot;);</span>
    }
    
<span class="pc bpc" id="L668" title="1 of 2 branches missed.">    if (!(m_Classifier instanceof WeightedInstancesHandler) &amp;&amp;</span>
<span class="nc bnc" id="L669" title="All 2 branches missed.">	!m_UseResampling) {</span>
<span class="nc" id="L670">      m_UseResampling = true;</span>
    }

    // can classifier handle the data?
<span class="fc" id="L674">    getCapabilities().testWithFail(data);</span>

<span class="pc bpc" id="L676" title="1 of 2 branches missed.">    if (m_Debug) {</span>
<span class="nc" id="L677">      System.err.println(&quot;Creating copy of the training data&quot;);</span>
    }

    // remove instances with missing class
<span class="fc" id="L681">    data = new Instances(data);</span>
<span class="fc" id="L682">    data.deleteWithMissingClass();</span>
    
    // only class? -&gt; build ZeroR model
<span class="fc bfc" id="L685" title="All 2 branches covered.">    if (data.numAttributes() == 1) {</span>
<span class="fc" id="L686">      System.err.println(</span>
<span class="fc" id="L687">	  &quot;Cannot build model (only class attribute present in data!), &quot;</span>
	  + &quot;using ZeroR model instead!&quot;);
<span class="fc" id="L689">      m_ZeroR = new weka.classifiers.rules.ZeroR();</span>
<span class="fc" id="L690">      m_ZeroR.buildClassifier(data);</span>
<span class="fc" id="L691">      return;</span>
    }
    else {
<span class="fc" id="L694">      m_ZeroR = null;</span>
    }
    
<span class="fc" id="L697">    m_NumClasses = data.numClasses();</span>
<span class="fc" id="L698">    m_ClassAttribute = data.classAttribute();</span>

    // Create the base classifiers
<span class="pc bpc" id="L701" title="1 of 2 branches missed.">    if (m_Debug) {</span>
<span class="nc" id="L702">      System.err.println(&quot;Creating base classifiers&quot;);</span>
    }
<span class="fc" id="L704">    m_Classifiers = new Classifier [m_NumClasses][];</span>
<span class="fc bfc" id="L705" title="All 2 branches covered.">    for (int j = 0; j &lt; m_NumClasses; j++) {</span>
<span class="fc" id="L706">      m_Classifiers[j] = Classifier.makeCopies(m_Classifier,</span>
<span class="fc" id="L707">					       getNumIterations());</span>
    }

    // Do we want to select the appropriate number of iterations
    // using cross-validation?
<span class="fc" id="L712">    int bestNumIterations = getNumIterations();</span>
<span class="pc bpc" id="L713" title="1 of 2 branches missed.">    if (m_NumFolds &gt; 1) {</span>
<span class="nc bnc" id="L714" title="All 2 branches missed.">      if (m_Debug) {</span>
<span class="nc" id="L715">	System.err.println(&quot;Processing first fold.&quot;);</span>
      }

      // Array for storing the results
<span class="nc" id="L719">      double[] results = new double[getNumIterations()];</span>

      // Iterate throught the cv-runs
<span class="nc bnc" id="L722" title="All 2 branches missed.">      for (int r = 0; r &lt; m_NumRuns; r++) {</span>

	// Stratify the data
<span class="nc" id="L725">	data.randomize(m_RandomInstance);</span>
<span class="nc" id="L726">	data.stratify(m_NumFolds);</span>
	
	// Perform the cross-validation
<span class="nc bnc" id="L729" title="All 2 branches missed.">	for (int i = 0; i &lt; m_NumFolds; i++) {</span>
	  
	  // Get train and test folds
<span class="nc" id="L732">	  Instances train = data.trainCV(m_NumFolds, i, m_RandomInstance);</span>
<span class="nc" id="L733">	  Instances test = data.testCV(m_NumFolds, i);</span>
	  
	  // Make class numeric
<span class="nc" id="L736">	  Instances trainN = new Instances(train);</span>
<span class="nc" id="L737">	  trainN.setClassIndex(-1);</span>
<span class="nc" id="L738">	  trainN.deleteAttributeAt(classIndex);</span>
<span class="nc" id="L739">	  trainN.insertAttributeAt(new Attribute(&quot;'pseudo class'&quot;), classIndex);</span>
<span class="nc" id="L740">	  trainN.setClassIndex(classIndex);</span>
<span class="nc" id="L741">	  m_NumericClassData = new Instances(trainN, 0);</span>
	  
	  // Get class values
<span class="nc" id="L744">	  int numInstances = train.numInstances();</span>
<span class="nc" id="L745">	  double [][] trainFs = new double [numInstances][m_NumClasses];</span>
<span class="nc" id="L746">	  double [][] trainYs = new double [numInstances][m_NumClasses];</span>
<span class="nc bnc" id="L747" title="All 2 branches missed.">	  for (int j = 0; j &lt; m_NumClasses; j++) {</span>
<span class="nc bnc" id="L748" title="All 2 branches missed.">	    for (int k = 0; k &lt; numInstances; k++) {</span>
<span class="nc bnc" id="L749" title="All 2 branches missed.">	      trainYs[k][j] = (train.instance(k).classValue() == j) ? </span>
<span class="nc" id="L750">		1.0 - m_Offset: 0.0 + (m_Offset / (double)m_NumClasses);</span>
	    }
	  }
	  
	  // Perform iterations
<span class="nc" id="L755">	  double[][] probs = initialProbs(numInstances);</span>
<span class="nc" id="L756">	  m_NumGenerated = 0;</span>
<span class="nc" id="L757">	  double sumOfWeights = train.sumOfWeights();</span>
<span class="nc bnc" id="L758" title="All 2 branches missed.">	  for (int j = 0; j &lt; getNumIterations(); j++) {</span>
<span class="nc" id="L759">	    performIteration(trainYs, trainFs, probs, trainN, sumOfWeights);</span>
<span class="nc" id="L760">	    Evaluation eval = new Evaluation(train);</span>
<span class="nc" id="L761">	    eval.evaluateModel(this, test);</span>
<span class="nc" id="L762">	    results[j] += eval.correct();</span>
	  }
	}
      }
      
      // Find the number of iterations with the lowest error
<span class="nc" id="L768">      double bestResult = -Double.MAX_VALUE;</span>
<span class="nc bnc" id="L769" title="All 2 branches missed.">      for (int j = 0; j &lt; getNumIterations(); j++) {</span>
<span class="nc bnc" id="L770" title="All 2 branches missed.">	if (results[j] &gt; bestResult) {</span>
<span class="nc" id="L771">	  bestResult = results[j];</span>
<span class="nc" id="L772">	  bestNumIterations = j;</span>
	}
      }
<span class="nc bnc" id="L775" title="All 2 branches missed.">      if (m_Debug) {</span>
<span class="nc" id="L776">	System.err.println(&quot;Best result for &quot; + </span>
<span class="nc" id="L777">			   bestNumIterations + &quot; iterations: &quot; +</span>
<span class="nc" id="L778">			   bestResult);</span>
      }
    }

    // Build classifier on all the data
<span class="fc" id="L783">    int numInstances = data.numInstances();</span>
<span class="fc" id="L784">    double [][] trainFs = new double [numInstances][m_NumClasses];</span>
<span class="fc" id="L785">    double [][] trainYs = new double [numInstances][m_NumClasses];</span>
<span class="fc bfc" id="L786" title="All 2 branches covered.">    for (int j = 0; j &lt; m_NumClasses; j++) {</span>
<span class="fc bfc" id="L787" title="All 2 branches covered.">      for (int i = 0, k = 0; i &lt; numInstances; i++, k++) {</span>
<span class="fc bfc" id="L788" title="All 2 branches covered.">	trainYs[i][j] = (data.instance(k).classValue() == j) ? </span>
<span class="fc" id="L789">	  1.0 - m_Offset: 0.0 + (m_Offset / (double)m_NumClasses);</span>
      }
    }
    
    // Make class numeric
<span class="fc" id="L794">    data.setClassIndex(-1);</span>
<span class="fc" id="L795">    data.deleteAttributeAt(classIndex);</span>
<span class="fc" id="L796">    data.insertAttributeAt(new Attribute(&quot;'pseudo class'&quot;), classIndex);</span>
<span class="fc" id="L797">    data.setClassIndex(classIndex);</span>
<span class="fc" id="L798">    m_NumericClassData = new Instances(data, 0);</span>
	
    // Perform iterations
<span class="fc" id="L801">    double[][] probs = initialProbs(numInstances);</span>
<span class="fc" id="L802">    double logLikelihood = logLikelihood(trainYs, probs);</span>
<span class="fc" id="L803">    m_NumGenerated = 0;</span>
<span class="pc bpc" id="L804" title="1 of 2 branches missed.">    if (m_Debug) {</span>
<span class="nc" id="L805">      System.err.println(&quot;Avg. log-likelihood: &quot; + logLikelihood);</span>
    }
<span class="fc" id="L807">    double sumOfWeights = data.sumOfWeights();</span>
<span class="fc bfc" id="L808" title="All 2 branches covered.">    for (int j = 0; j &lt; bestNumIterations; j++) {</span>
<span class="fc" id="L809">      double previousLoglikelihood = logLikelihood;</span>
<span class="fc" id="L810">      performIteration(trainYs, trainFs, probs, data, sumOfWeights);</span>
<span class="fc" id="L811">      logLikelihood = logLikelihood(trainYs, probs);</span>
<span class="pc bpc" id="L812" title="1 of 2 branches missed.">      if (m_Debug) {</span>
<span class="nc" id="L813">	System.err.println(&quot;Avg. log-likelihood: &quot; + logLikelihood);</span>
      }
<span class="pc bpc" id="L815" title="1 of 2 branches missed.">      if (Math.abs(previousLoglikelihood - logLikelihood) &lt; m_Precision) {</span>
<span class="nc" id="L816">	return;</span>
      }
    }
<span class="fc" id="L819">  }</span>

  /**
   * Gets the intial class probabilities.
   * 
   * @param numInstances the number of instances
   * @return the initial class probabilities
   */
  private double[][] initialProbs(int numInstances) {

<span class="fc" id="L829">    double[][] probs = new double[numInstances][m_NumClasses];</span>
<span class="fc bfc" id="L830" title="All 2 branches covered.">    for (int i = 0; i &lt; numInstances; i++) {</span>
<span class="fc bfc" id="L831" title="All 2 branches covered.">      for (int j = 0 ; j &lt; m_NumClasses; j++) {</span>
<span class="fc" id="L832">	probs[i][j] = 1.0 / m_NumClasses;</span>
      }
    }
<span class="fc" id="L835">    return probs;</span>
  }

  /**
   * Computes loglikelihood given class values
   * and estimated probablities.
   * 
   * @param trainYs class values
   * @param probs estimated probabilities
   * @return the computed loglikelihood
   */
  private double logLikelihood(double[][] trainYs, double[][] probs) {

<span class="fc" id="L848">    double logLikelihood = 0;</span>
<span class="fc bfc" id="L849" title="All 2 branches covered.">    for (int i = 0; i &lt; trainYs.length; i++) {</span>
<span class="fc bfc" id="L850" title="All 2 branches covered.">      for (int j = 0; j &lt; m_NumClasses; j++) {</span>
<span class="fc bfc" id="L851" title="All 2 branches covered.">	if (trainYs[i][j] == 1.0 - m_Offset) {</span>
<span class="fc" id="L852">	  logLikelihood -= Math.log(probs[i][j]);</span>
	}
      }
    }
<span class="fc" id="L856">    return logLikelihood / (double)trainYs.length;</span>
  }

  /**
   * Performs one boosting iteration.
   * 
   * @param trainYs class values
   * @param trainFs F scores
   * @param probs probabilities
   * @param data the data to run the iteration on
   * @param origSumOfWeights the original sum of weights
   * @throws Exception in case base classifiers run into problems
   */
  private void performIteration(double[][] trainYs,
				double[][] trainFs,
				double[][] probs,
				Instances data,
				double origSumOfWeights) throws Exception {

<span class="pc bpc" id="L875" title="1 of 2 branches missed.">    if (m_Debug) {</span>
<span class="nc" id="L876">      System.err.println(&quot;Training classifier &quot; + (m_NumGenerated + 1));</span>
    }

    // Build the new models
<span class="fc bfc" id="L880" title="All 2 branches covered.">    for (int j = 0; j &lt; m_NumClasses; j++) {</span>
<span class="pc bpc" id="L881" title="1 of 2 branches missed.">      if (m_Debug) {</span>
<span class="nc" id="L882">	System.err.println(&quot;\t...for class &quot; + (j + 1)</span>
<span class="nc" id="L883">			   + &quot; (&quot; + m_ClassAttribute.name() </span>
<span class="nc" id="L884">			   + &quot;=&quot; + m_ClassAttribute.value(j) + &quot;)&quot;);</span>
      }
    
      // Make copy because we want to save the weights
<span class="fc" id="L888">      Instances boostData = new Instances(data);</span>
      
      // Set instance pseudoclass and weights
<span class="fc bfc" id="L891" title="All 2 branches covered.">      for (int i = 0; i &lt; probs.length; i++) {</span>

	// Compute response and weight
<span class="fc" id="L894">	double p = probs[i][j];</span>
<span class="fc" id="L895">	double z, actual = trainYs[i][j];</span>
<span class="fc bfc" id="L896" title="All 2 branches covered.">	if (actual == 1 - m_Offset) {</span>
<span class="fc" id="L897">	  z = 1.0 / p;</span>
<span class="fc bfc" id="L898" title="All 2 branches covered.">	  if (z &gt; Z_MAX) { // threshold</span>
<span class="fc" id="L899">	    z = Z_MAX;</span>
	  }
	} else {
<span class="fc" id="L902">	  z = -1.0 / (1.0 - p);</span>
<span class="fc bfc" id="L903" title="All 2 branches covered.">	  if (z &lt; -Z_MAX) { // threshold</span>
<span class="fc" id="L904">	    z = -Z_MAX;</span>
	  }
	}
<span class="fc" id="L907">	double w = (actual - p) / z;</span>

	// Set values for instance
<span class="fc" id="L910">	Instance current = boostData.instance(i);</span>
<span class="fc" id="L911">	current.setValue(boostData.classIndex(), z);</span>
<span class="fc" id="L912">	current.setWeight(current.weight() * w);</span>
      }
      
      // Scale the weights (helps with some base learners)
<span class="fc" id="L916">      double sumOfWeights = boostData.sumOfWeights();</span>
<span class="fc" id="L917">      double scalingFactor = (double)origSumOfWeights / sumOfWeights;</span>
<span class="fc bfc" id="L918" title="All 2 branches covered.">      for (int i = 0; i &lt; probs.length; i++) {</span>
<span class="fc" id="L919">	Instance current = boostData.instance(i);</span>
<span class="fc" id="L920">	current.setWeight(current.weight() * scalingFactor);</span>
      }

      // Select instances to train the classifier on
<span class="fc" id="L924">      Instances trainData = boostData;</span>
<span class="pc bpc" id="L925" title="1 of 2 branches missed.">      if (m_WeightThreshold &lt; 100) {</span>
<span class="nc" id="L926">	trainData = selectWeightQuantile(boostData, </span>
<span class="nc" id="L927">					 (double)m_WeightThreshold / 100);</span>
      } else {
<span class="pc bpc" id="L929" title="1 of 2 branches missed.">	if (m_UseResampling) {</span>
<span class="nc" id="L930">	  double[] weights = new double[boostData.numInstances()];</span>
<span class="nc bnc" id="L931" title="All 2 branches missed.">	  for (int kk = 0; kk &lt; weights.length; kk++) {</span>
<span class="nc" id="L932">	    weights[kk] = boostData.instance(kk).weight();</span>
	  }
<span class="nc" id="L934">	  trainData = boostData.resampleWithWeights(m_RandomInstance, </span>
<span class="nc" id="L935">						    weights);</span>
	}
      }
      
      // Build the classifier
<span class="fc" id="L940">      m_Classifiers[j][m_NumGenerated].buildClassifier(trainData);</span>
    }      
    
    // Evaluate / increment trainFs from the classifier
<span class="fc bfc" id="L944" title="All 2 branches covered.">    for (int i = 0; i &lt; trainFs.length; i++) {</span>
<span class="fc" id="L945">      double [] pred = new double [m_NumClasses];</span>
<span class="fc" id="L946">      double predSum = 0;</span>
<span class="fc bfc" id="L947" title="All 2 branches covered.">      for (int j = 0; j &lt; m_NumClasses; j++) {</span>
<span class="fc" id="L948">	pred[j] = m_Shrinkage * m_Classifiers[j][m_NumGenerated]</span>
<span class="fc" id="L949">	  .classifyInstance(data.instance(i));</span>
<span class="fc" id="L950">	predSum += pred[j];</span>
      }
<span class="fc" id="L952">      predSum /= m_NumClasses;</span>
<span class="fc bfc" id="L953" title="All 2 branches covered.">      for (int j = 0; j &lt; m_NumClasses; j++) {</span>
<span class="fc" id="L954">	trainFs[i][j] += (pred[j] - predSum) * (m_NumClasses - 1) </span>
<span class="fc" id="L955">	  / m_NumClasses;</span>
      }
    }
<span class="fc" id="L958">    m_NumGenerated++;</span>
    
    // Compute the current probability estimates
<span class="fc bfc" id="L961" title="All 2 branches covered.">    for (int i = 0; i &lt; trainYs.length; i++) {</span>
<span class="fc" id="L962">      probs[i] = probs(trainFs[i]);</span>
    }
<span class="fc" id="L964">  }</span>

  /**
   * Returns the array of classifiers that have been built.
   * 
   * @return the built classifiers
   */
  public Classifier[][] classifiers() {

<span class="nc" id="L973">    Classifier[][] classifiers = </span>
<span class="nc" id="L974">      new Classifier[m_NumClasses][m_NumGenerated];</span>
<span class="nc bnc" id="L975" title="All 2 branches missed.">    for (int j = 0; j &lt; m_NumClasses; j++) {</span>
<span class="nc bnc" id="L976" title="All 2 branches missed.">      for (int i = 0; i &lt; m_NumGenerated; i++) {</span>
<span class="nc" id="L977">	classifiers[j][i] = m_Classifiers[j][i];</span>
      }
    }
<span class="nc" id="L980">    return classifiers;</span>
  }

  /**
   * Computes probabilities from F scores
   * 
   * @param Fs the F scores
   * @return the computed probabilities
   */
  private double[] probs(double[] Fs) {

<span class="fc" id="L991">    double maxF = -Double.MAX_VALUE;</span>
<span class="fc bfc" id="L992" title="All 2 branches covered.">    for (int i = 0; i &lt; Fs.length; i++) {</span>
<span class="fc bfc" id="L993" title="All 2 branches covered.">      if (Fs[i] &gt; maxF) {</span>
<span class="fc" id="L994">	maxF = Fs[i];</span>
      }
    }
<span class="fc" id="L997">    double sum = 0;</span>
<span class="fc" id="L998">    double[] probs = new double[Fs.length];</span>
<span class="fc bfc" id="L999" title="All 2 branches covered.">    for (int i = 0; i &lt; Fs.length; i++) {</span>
<span class="fc" id="L1000">      probs[i] = Math.exp(Fs[i] - maxF);</span>
<span class="fc" id="L1001">      sum += probs[i];</span>
    }
<span class="fc" id="L1003">    Utils.normalize(probs, sum);</span>
<span class="fc" id="L1004">    return probs;</span>
  }
    
  /**
   * Calculates the class membership probabilities for the given test instance.
   *
   * @param instance the instance to be classified
   * @return predicted class probability distribution
   * @throws Exception if instance could not be classified
   * successfully
   */
  public double [] distributionForInstance(Instance instance) 
    throws Exception {

    // default model?
<span class="fc bfc" id="L1019" title="All 2 branches covered.">    if (m_ZeroR != null) {</span>
<span class="fc" id="L1020">      return m_ZeroR.distributionForInstance(instance);</span>
    }
    
<span class="fc" id="L1023">    instance = (Instance)instance.copy();</span>
<span class="fc" id="L1024">    instance.setDataset(m_NumericClassData);</span>
<span class="fc" id="L1025">    double [] pred = new double [m_NumClasses];</span>
<span class="fc" id="L1026">    double [] Fs = new double [m_NumClasses]; </span>
<span class="fc bfc" id="L1027" title="All 2 branches covered.">    for (int i = 0; i &lt; m_NumGenerated; i++) {</span>
<span class="fc" id="L1028">      double predSum = 0;</span>
<span class="fc bfc" id="L1029" title="All 2 branches covered.">      for (int j = 0; j &lt; m_NumClasses; j++) {</span>
<span class="fc" id="L1030">	pred[j] = m_Shrinkage * m_Classifiers[j][i].classifyInstance(instance);</span>
<span class="fc" id="L1031">	predSum += pred[j];</span>
      }
<span class="fc" id="L1033">      predSum /= m_NumClasses;</span>
<span class="fc bfc" id="L1034" title="All 2 branches covered.">      for (int j = 0; j &lt; m_NumClasses; j++) {</span>
<span class="fc" id="L1035">	Fs[j] += (pred[j] - predSum) * (m_NumClasses - 1) </span>
<span class="fc" id="L1036">	  / m_NumClasses;</span>
      }
    }

<span class="fc" id="L1040">    return probs(Fs);</span>
  }

  /**
   * Returns the boosted model as Java source code.
   *
   * @param className the classname in the generated code
   * @return the tree as Java source code
   * @throws Exception if something goes wrong
   */
  public String toSource(String className) throws Exception {

<span class="nc bnc" id="L1052" title="All 2 branches missed.">    if (m_NumGenerated == 0) {</span>
<span class="nc" id="L1053">      throw new Exception(&quot;No model built yet&quot;);</span>
    }
<span class="nc bnc" id="L1055" title="All 2 branches missed.">    if (!(m_Classifiers[0][0] instanceof Sourcable)) {</span>
<span class="nc" id="L1056">      throw new Exception(&quot;Base learner &quot; + m_Classifier.getClass().getName()</span>
<span class="nc" id="L1057">			  + &quot; is not Sourcable&quot;);</span>
    }

<span class="nc" id="L1060">    StringBuffer text = new StringBuffer(&quot;class &quot;);</span>
<span class="nc" id="L1061">    text.append(className).append(&quot; {\n\n&quot;);</span>
<span class="nc" id="L1062">    text.append(&quot;  private static double RtoP(double []R, int j) {\n&quot;+</span>
		&quot;    double Rcenter = 0;\n&quot;+
		&quot;    for (int i = 0; i &lt; R.length; i++) {\n&quot;+
		&quot;      Rcenter += R[i];\n&quot;+
		&quot;    }\n&quot;+
		&quot;    Rcenter /= R.length;\n&quot;+
		&quot;    double Rsum = 0;\n&quot;+
		&quot;    for (int i = 0; i &lt; R.length; i++) {\n&quot;+
		&quot;      Rsum += Math.exp(R[i] - Rcenter);\n&quot;+
		&quot;    }\n&quot;+
		&quot;    return Math.exp(R[j]) / Rsum;\n&quot;+
		&quot;  }\n\n&quot;);

<span class="nc" id="L1075">    text.append(&quot;  public static double classify(Object[] i) {\n&quot; +</span>
                &quot;    double [] d = distribution(i);\n&quot; +
                &quot;    double maxV = d[0];\n&quot; +
		&quot;    int maxI = 0;\n&quot;+
<span class="nc" id="L1079">		&quot;    for (int j = 1; j &lt; &quot; + m_NumClasses + &quot;; j++) {\n&quot;+</span>
<span class="nc" id="L1080">		&quot;      if (d[j] &gt; maxV) { maxV = d[j]; maxI = j; }\n&quot;+</span>
<span class="nc" id="L1081">		&quot;    }\n    return (double) maxI;\n  }\n\n&quot;);</span>

<span class="nc" id="L1083">    text.append(&quot;  public static double [] distribution(Object [] i) {\n&quot;);</span>
<span class="nc" id="L1084">    text.append(&quot;    double [] Fs = new double [&quot; + m_NumClasses + &quot;];\n&quot;);</span>
<span class="nc" id="L1085">    text.append(&quot;    double [] Fi = new double [&quot; + m_NumClasses + &quot;];\n&quot;);</span>
<span class="nc" id="L1086">    text.append(&quot;    double Fsum;\n&quot;);</span>
<span class="nc bnc" id="L1087" title="All 2 branches missed.">    for (int i = 0; i &lt; m_NumGenerated; i++) {</span>
<span class="nc" id="L1088">      text.append(&quot;    Fsum = 0;\n&quot;);</span>
<span class="nc bnc" id="L1089" title="All 2 branches missed.">      for (int j = 0; j &lt; m_NumClasses; j++) {</span>
<span class="nc" id="L1090">	text.append(&quot;    Fi[&quot; + j + &quot;] = &quot; + className + '_' +j + '_' + i </span>
<span class="nc" id="L1091">		    + &quot;.classify(i); Fsum += Fi[&quot; + j + &quot;];\n&quot;);</span>
      }
<span class="nc" id="L1093">      text.append(&quot;    Fsum /= &quot; + m_NumClasses + &quot;;\n&quot;);</span>
<span class="nc" id="L1094">      text.append(&quot;    for (int j = 0; j &lt; &quot; + m_NumClasses + &quot;; j++) {&quot;);</span>
<span class="nc" id="L1095">      text.append(&quot; Fs[j] += (Fi[j] - Fsum) * &quot;</span>
<span class="nc" id="L1096">		  + (m_NumClasses - 1) + &quot; / &quot; + m_NumClasses + &quot;; }\n&quot;);</span>
    }
    
<span class="nc" id="L1099">    text.append(&quot;    double [] dist = new double [&quot; + m_NumClasses + &quot;];\n&quot; +</span>
<span class="nc" id="L1100">		&quot;    for (int j = 0; j &lt; &quot; + m_NumClasses + &quot;; j++) {\n&quot;+</span>
<span class="nc" id="L1101">		&quot;      dist[j] = RtoP(Fs, j);\n&quot;+</span>
<span class="nc" id="L1102">		&quot;    }\n    return dist;\n&quot;);</span>
<span class="nc" id="L1103">    text.append(&quot;  }\n}\n&quot;);</span>

<span class="nc bnc" id="L1105" title="All 2 branches missed.">    for (int i = 0; i &lt; m_Classifiers.length; i++) {</span>
<span class="nc bnc" id="L1106" title="All 2 branches missed.">      for (int j = 0; j &lt; m_Classifiers[i].length; j++) {</span>
<span class="nc" id="L1107">	text.append(((Sourcable)m_Classifiers[i][j])</span>
<span class="nc" id="L1108">		    .toSource(className + '_' + i + '_' + j));</span>
      }
    }
<span class="nc" id="L1111">    return text.toString();</span>
  }

  /**
   * Returns description of the boosted classifier.
   *
   * @return description of the boosted classifier as a string
   */
  public String toString() {
    
    // only ZeroR model?
<span class="pc bpc" id="L1122" title="1 of 2 branches missed.">    if (m_ZeroR != null) {</span>
<span class="nc" id="L1123">      StringBuffer buf = new StringBuffer();</span>
<span class="nc" id="L1124">      buf.append(this.getClass().getName().replaceAll(&quot;.*\\.&quot;, &quot;&quot;) + &quot;\n&quot;);</span>
<span class="nc" id="L1125">      buf.append(this.getClass().getName().replaceAll(&quot;.*\\.&quot;, &quot;&quot;).replaceAll(&quot;.&quot;, &quot;=&quot;) + &quot;\n\n&quot;);</span>
<span class="nc" id="L1126">      buf.append(&quot;Warning: No model could be built, hence ZeroR model is used:\n\n&quot;);</span>
<span class="nc" id="L1127">      buf.append(m_ZeroR.toString());</span>
<span class="nc" id="L1128">      return buf.toString();</span>
    }
    
<span class="fc" id="L1131">    StringBuffer text = new StringBuffer();</span>
    
<span class="pc bpc" id="L1133" title="1 of 2 branches missed.">    if (m_NumGenerated == 0) {</span>
<span class="fc" id="L1134">      text.append(&quot;LogitBoost: No model built yet.&quot;);</span>
      //      text.append(m_Classifiers[0].toString()+&quot;\n&quot;);
    } else {
<span class="nc" id="L1137">      text.append(&quot;LogitBoost: Base classifiers and their weights: \n&quot;);</span>
<span class="nc bnc" id="L1138" title="All 2 branches missed.">      for (int i = 0; i &lt; m_NumGenerated; i++) {</span>
<span class="nc" id="L1139">	text.append(&quot;\nIteration &quot;+(i+1));</span>
<span class="nc bnc" id="L1140" title="All 2 branches missed.">	for (int j = 0; j &lt; m_NumClasses; j++) {</span>
<span class="nc" id="L1141">	  text.append(&quot;\n\tClass &quot; + (j + 1) </span>
<span class="nc" id="L1142">		      + &quot; (&quot; + m_ClassAttribute.name() </span>
<span class="nc" id="L1143">		      + &quot;=&quot; + m_ClassAttribute.value(j) + &quot;)\n\n&quot;</span>
<span class="nc" id="L1144">		      + m_Classifiers[j][i].toString() + &quot;\n&quot;);</span>
	}
      }
<span class="nc" id="L1147">      text.append(&quot;Number of performed iterations: &quot; +</span>
<span class="nc" id="L1148">		    m_NumGenerated + &quot;\n&quot;);</span>
    }
    
<span class="fc" id="L1151">    return text.toString();</span>
  }
  
  /**
   * Returns the revision string.
   * 
   * @return		the revision
   */
  public String getRevision() {
<span class="nc" id="L1160">    return RevisionUtils.extract(&quot;$Revision: 9371 $&quot;);</span>
  }

  /**
   * Main method for testing this class.
   *
   * @param argv the options
   */
  public static void main(String [] argv) {
<span class="nc" id="L1169">    runClassifier(new LogitBoost(), argv);</span>
<span class="nc" id="L1170">  }</span>
}
</pre><div class="footer"><span class="right">Created with <a href="http://www.eclemma.org/jacoco">JaCoCo</a> 0.7.2.201409121644</span>AllTests (Nov 28, 2015 2:34:31 PM)</div></body></html>
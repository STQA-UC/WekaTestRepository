<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><meta http-equiv="Content-Type" content="text/html;charset=UTF-8"/><link rel="stylesheet" href="../../../.resources/report.css" type="text/css"/><link rel="shortcut icon" href="../../../.resources/report.gif" type="image/gif"/><title>IBk.java</title><link rel="stylesheet" href="../../../.resources/prettify.css" type="text/css"/><script type="text/javascript" src="../../../.resources/prettify.js"></script></head><body onload="window['PR_TAB_WIDTH']=4;prettyPrint()"><div class="breadcrumb" id="breadcrumb"><span class="info"><a href="../../../.sessions.html" class="el_session">Sessions</a></span><a href="../../../index.html" class="el_report">AllTests (Nov 28, 2015 2:34:31 PM)</a> &gt; <a href="../../index.html" class="el_group">wekaproject</a> &gt; <a href="../index.html" class="el_bundle">src/src/main/java</a> &gt; <a href="index.source.html" class="el_package">weka.classifiers.lazy</a> &gt; <span class="el_source">IBk.java</span></div><h1>IBk.java</h1><pre class="source lang-java linenums">/*
 *    This program is free software; you can redistribute it and/or modify
 *    it under the terms of the GNU General Public License as published by
 *    the Free Software Foundation; either version 2 of the License, or
 *    (at your option) any later version.
 *
 *    This program is distributed in the hope that it will be useful,
 *    but WITHOUT ANY WARRANTY; without even the implied warranty of
 *    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 *    GNU General Public License for more details.
 *
 *    You should have received a copy of the GNU General Public License
 *    along with this program; if not, write to the Free Software
 *    Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
 */

/*
 *    IBk.java
 *    Copyright (C) 1999 University of Waikato, Hamilton, New Zealand
 *
 */

package weka.classifiers.lazy;

import weka.classifiers.Classifier;
import weka.classifiers.UpdateableClassifier;
import weka.classifiers.rules.ZeroR;
import weka.core.Attribute;
import weka.core.Capabilities;
import weka.core.Instance;
import weka.core.Instances;
import weka.core.neighboursearch.LinearNNSearch;
import weka.core.neighboursearch.NearestNeighbourSearch;
import weka.core.Option;
import weka.core.OptionHandler;
import weka.core.RevisionUtils;
import weka.core.SelectedTag;
import weka.core.Tag;
import weka.core.TechnicalInformation;
import weka.core.TechnicalInformationHandler;
import weka.core.Utils;
import weka.core.WeightedInstancesHandler;
import weka.core.Capabilities.Capability;
import weka.core.TechnicalInformation.Field;
import weka.core.TechnicalInformation.Type;
import weka.core.AdditionalMeasureProducer;

import java.util.Enumeration;
import java.util.Vector;

/**
 &lt;!-- globalinfo-start --&gt;
 * K-nearest neighbours classifier. Can select appropriate value of K based on cross-validation. Can also do distance weighting.&lt;br/&gt;
 * &lt;br/&gt;
 * For more information, see&lt;br/&gt;
 * &lt;br/&gt;
 * D. Aha, D. Kibler (1991). Instance-based learning algorithms. Machine Learning. 6:37-66.
 * &lt;p/&gt;
 &lt;!-- globalinfo-end --&gt;
 * 
 &lt;!-- technical-bibtex-start --&gt;
 * BibTeX:
 * &lt;pre&gt;
 * &amp;#64;article{Aha1991,
 *    author = {D. Aha and D. Kibler},
 *    journal = {Machine Learning},
 *    pages = {37-66},
 *    title = {Instance-based learning algorithms},
 *    volume = {6},
 *    year = {1991}
 * }
 * &lt;/pre&gt;
 * &lt;p/&gt;
 &lt;!-- technical-bibtex-end --&gt;
 *
 &lt;!-- options-start --&gt;
 * Valid options are: &lt;p/&gt;
 * 
 * &lt;pre&gt; -I
 *  Weight neighbours by the inverse of their distance
 *  (use when k &amp;gt; 1)&lt;/pre&gt;
 * 
 * &lt;pre&gt; -F
 *  Weight neighbours by 1 - their distance
 *  (use when k &amp;gt; 1)&lt;/pre&gt;
 * 
 * &lt;pre&gt; -K &amp;lt;number of neighbors&amp;gt;
 *  Number of nearest neighbours (k) used in classification.
 *  (Default = 1)&lt;/pre&gt;
 * 
 * &lt;pre&gt; -E
 *  Minimise mean squared error rather than mean absolute
 *  error when using -X option with numeric prediction.&lt;/pre&gt;
 * 
 * &lt;pre&gt; -W &amp;lt;window size&amp;gt;
 *  Maximum number of training instances maintained.
 *  Training instances are dropped FIFO. (Default = no window)&lt;/pre&gt;
 * 
 * &lt;pre&gt; -X
 *  Select the number of nearest neighbours between 1
 *  and the k value specified using hold-one-out evaluation
 *  on the training data (use when k &amp;gt; 1)&lt;/pre&gt;
 * 
 * &lt;pre&gt; -A
 *  The nearest neighbour search algorithm to use (default: weka.core.neighboursearch.LinearNNSearch).
 * &lt;/pre&gt;
 * 
 &lt;!-- options-end --&gt;
 *
 * @author Stuart Inglis (singlis@cs.waikato.ac.nz)
 * @author Len Trigg (trigg@cs.waikato.ac.nz)
 * @author Eibe Frank (eibe@cs.waikato.ac.nz)
 * @version $Revision: 6573 $
 */
<span class="fc" id="L115">public class IBk </span>
  extends Classifier 
  implements OptionHandler, UpdateableClassifier, WeightedInstancesHandler,
             TechnicalInformationHandler, AdditionalMeasureProducer {

  /** for serialization. */
  static final long serialVersionUID = -3080186098777067172L;

  /** The training instances used for classification. */
  protected Instances m_Train;

  /** The number of class values (or 1 if predicting numeric). */
  protected int m_NumClasses;

  /** The class attribute type. */
  protected int m_ClassType;

  /** The number of neighbours to use for classification (currently). */
  protected int m_kNN;

  /**
   * The value of kNN provided by the user. This may differ from
   * m_kNN if cross-validation is being used.
   */
  protected int m_kNNUpper;

  /**
   * Whether the value of k selected by cross validation has
   * been invalidated by a change in the training instances.
   */
  protected boolean m_kNNValid;

  /**
   * The maximum number of training instances allowed. When
   * this limit is reached, old training instances are removed,
   * so the training data is &quot;windowed&quot;. Set to 0 for unlimited
   * numbers of instances.
   */
  protected int m_WindowSize;

  /** Whether the neighbours should be distance-weighted. */
  protected int m_DistanceWeighting;

  /** Whether to select k by cross validation. */
  protected boolean m_CrossValidate;

  /**
   * Whether to minimise mean squared error rather than mean absolute
   * error when cross-validating on numeric prediction tasks.
   */
  protected boolean m_MeanSquared;

  /** no weighting. */
  public static final int WEIGHT_NONE = 1;
  /** weight by 1/distance. */
  public static final int WEIGHT_INVERSE = 2;
  /** weight by 1-distance. */
  public static final int WEIGHT_SIMILARITY = 4;
  /** possible instance weighting methods. */
<span class="fc" id="L174">  public static final Tag [] TAGS_WEIGHTING = {</span>
<span class="fc" id="L175">    new Tag(WEIGHT_NONE, &quot;No distance weighting&quot;),</span>
<span class="fc" id="L176">    new Tag(WEIGHT_INVERSE, &quot;Weight by 1/distance&quot;),</span>
<span class="fc" id="L177">    new Tag(WEIGHT_SIMILARITY, &quot;Weight by 1-distance&quot;)</span>
  };
  
  /** for nearest-neighbor search. */
<span class="pc" id="L181">  protected NearestNeighbourSearch m_NNSearch = new LinearNNSearch();</span>

  /** The number of attributes the contribute to a prediction. */
  protected double m_NumAttributesUsed;
  
  /** Default ZeroR model to use when there are no training instances */
  protected ZeroR m_defaultModel;
  
  /**
   * IBk classifier. Simple instance-based learner that uses the class
   * of the nearest k training instances for the class of the test
   * instances.
   *
   * @param k the number of nearest neighbors to use for prediction
   */
<span class="nc" id="L196">  public IBk(int k) {</span>

<span class="nc" id="L198">    init();</span>
<span class="nc" id="L199">    setKNN(k);</span>
<span class="nc" id="L200">  }  </span>

  /**
   * IB1 classifer. Instance-based learner. Predicts the class of the
   * single nearest training instance for each test instance.
   */
<span class="fc" id="L206">  public IBk() {</span>

<span class="fc" id="L208">    init();</span>
<span class="fc" id="L209">  }</span>
  
  /**
   * Returns a string describing classifier.
   * @return a description suitable for
   * displaying in the explorer/experimenter gui
   */
  public String globalInfo() {

<span class="nc" id="L218">    return  &quot;K-nearest neighbours classifier. Can &quot;</span>
      + &quot;select appropriate value of K based on cross-validation. Can also do &quot;
      + &quot;distance weighting.\n\n&quot;
      + &quot;For more information, see\n\n&quot;
<span class="nc" id="L222">      + getTechnicalInformation().toString();</span>
  }

  /**
   * Returns an instance of a TechnicalInformation object, containing 
   * detailed information about the technical background of this class,
   * e.g., paper reference or book this class is based on.
   * 
   * @return the technical information about this class
   */
  public TechnicalInformation getTechnicalInformation() {
    TechnicalInformation 	result;
    
<span class="nc" id="L235">    result = new TechnicalInformation(Type.ARTICLE);</span>
<span class="nc" id="L236">    result.setValue(Field.AUTHOR, &quot;D. Aha and D. Kibler&quot;);</span>
<span class="nc" id="L237">    result.setValue(Field.YEAR, &quot;1991&quot;);</span>
<span class="nc" id="L238">    result.setValue(Field.TITLE, &quot;Instance-based learning algorithms&quot;);</span>
<span class="nc" id="L239">    result.setValue(Field.JOURNAL, &quot;Machine Learning&quot;);</span>
<span class="nc" id="L240">    result.setValue(Field.VOLUME, &quot;6&quot;);</span>
<span class="nc" id="L241">    result.setValue(Field.PAGES, &quot;37-66&quot;);</span>
    
<span class="nc" id="L243">    return result;</span>
  }

  /**
   * Returns the tip text for this property.
   * @return tip text for this property suitable for
   * displaying in the explorer/experimenter gui
   */
  public String KNNTipText() {
<span class="nc" id="L252">    return &quot;The number of neighbours to use.&quot;;</span>
  }
  
  /**
   * Set the number of neighbours the learner is to use.
   *
   * @param k the number of neighbours.
   */
  public void setKNN(int k) {
<span class="fc" id="L261">    m_kNN = k;</span>
<span class="fc" id="L262">    m_kNNUpper = k;</span>
<span class="fc" id="L263">    m_kNNValid = false;</span>
<span class="fc" id="L264">  }</span>

  /**
   * Gets the number of neighbours the learner will use.
   *
   * @return the number of neighbours.
   */
  public int getKNN() {

<span class="fc" id="L273">    return m_kNN;</span>
  }

  /**
   * Returns the tip text for this property.
   * @return tip text for this property suitable for
   * displaying in the explorer/experimenter gui
   */
  public String windowSizeTipText() {
<span class="nc" id="L282">    return &quot;Gets the maximum number of instances allowed in the training &quot; +</span>
      &quot;pool. The addition of new instances above this value will result &quot; +
      &quot;in old instances being removed. A value of 0 signifies no limit &quot; +
      &quot;to the number of training instances.&quot;;
  }
  
  /**
   * Gets the maximum number of instances allowed in the training
   * pool. The addition of new instances above this value will result
   * in old instances being removed. A value of 0 signifies no limit
   * to the number of training instances.
   *
   * @return Value of WindowSize.
   */
  public int getWindowSize() {
    
<span class="nc" id="L298">    return m_WindowSize;</span>
  }
  
  /**
   * Sets the maximum number of instances allowed in the training
   * pool. The addition of new instances above this value will result
   * in old instances being removed. A value of 0 signifies no limit
   * to the number of training instances.
   *
   * @param newWindowSize Value to assign to WindowSize.
   */
  public void setWindowSize(int newWindowSize) {
    
<span class="fc" id="L311">    m_WindowSize = newWindowSize;</span>
<span class="fc" id="L312">  }</span>
  
  /**
   * Returns the tip text for this property.
   * @return tip text for this property suitable for
   * displaying in the explorer/experimenter gui
   */
  public String distanceWeightingTipText() {

<span class="nc" id="L321">    return &quot;Gets the distance weighting method used.&quot;;</span>
  }
  
  /**
   * Gets the distance weighting method used. Will be one of
   * WEIGHT_NONE, WEIGHT_INVERSE, or WEIGHT_SIMILARITY
   *
   * @return the distance weighting method used.
   */
  public SelectedTag getDistanceWeighting() {

<span class="nc" id="L332">    return new SelectedTag(m_DistanceWeighting, TAGS_WEIGHTING);</span>
  }
  
  /**
   * Sets the distance weighting method used. Values other than
   * WEIGHT_NONE, WEIGHT_INVERSE, or WEIGHT_SIMILARITY will be ignored.
   *
   * @param newMethod the distance weighting method to use
   */
  public void setDistanceWeighting(SelectedTag newMethod) {
    
<span class="pc bpc" id="L343" title="1 of 2 branches missed.">    if (newMethod.getTags() == TAGS_WEIGHTING) {</span>
<span class="fc" id="L344">      m_DistanceWeighting = newMethod.getSelectedTag().getID();</span>
    }
<span class="fc" id="L346">  }</span>
  
  /**
   * Returns the tip text for this property.
   * @return tip text for this property suitable for
   * displaying in the explorer/experimenter gui
   */
  public String meanSquaredTipText() {

<span class="nc" id="L355">    return &quot;Whether the mean squared error is used rather than mean &quot;</span>
      + &quot;absolute error when doing cross-validation for regression problems.&quot;;
  }

  /**
   * Gets whether the mean squared error is used rather than mean
   * absolute error when doing cross-validation.
   *
   * @return true if so.
   */
  public boolean getMeanSquared() {
    
<span class="fc" id="L367">    return m_MeanSquared;</span>
  }
  
  /**
   * Sets whether the mean squared error is used rather than mean
   * absolute error when doing cross-validation.
   *
   * @param newMeanSquared true if so.
   */
  public void setMeanSquared(boolean newMeanSquared) {
    
<span class="fc" id="L378">    m_MeanSquared = newMeanSquared;</span>
<span class="fc" id="L379">  }</span>
  
  /**
   * Returns the tip text for this property.
   * @return tip text for this property suitable for
   * displaying in the explorer/experimenter gui
   */
  public String crossValidateTipText() {

<span class="nc" id="L388">    return &quot;Whether hold-one-out cross-validation will be used &quot; +</span>
      &quot;to select the best k value.&quot;;
  }
  
  /**
   * Gets whether hold-one-out cross-validation will be used
   * to select the best k value.
   *
   * @return true if cross-validation will be used.
   */
  public boolean getCrossValidate() {
    
<span class="fc" id="L400">    return m_CrossValidate;</span>
  }
  
  /**
   * Sets whether hold-one-out cross-validation will be used
   * to select the best k value.
   *
   * @param newCrossValidate true if cross-validation should be used.
   */
  public void setCrossValidate(boolean newCrossValidate) {
    
<span class="fc" id="L411">    m_CrossValidate = newCrossValidate;</span>
<span class="fc" id="L412">  }</span>

  /**
   * Returns the tip text for this property.
   * @return tip text for this property suitable for
   * displaying in the explorer/experimenter gui
   */
  public String nearestNeighbourSearchAlgorithmTipText() {
<span class="nc" id="L420">    return &quot;The nearest neighbour search algorithm to use &quot; +</span>
    	   &quot;(Default: weka.core.neighboursearch.LinearNNSearch).&quot;;
  }
  
  /**
   * Returns the current nearestNeighbourSearch algorithm in use.
   * @return the NearestNeighbourSearch algorithm currently in use.
   */
  public NearestNeighbourSearch getNearestNeighbourSearchAlgorithm() {
<span class="nc" id="L429">    return m_NNSearch;</span>
  }
  
  /**
   * Sets the nearestNeighbourSearch algorithm to be used for finding nearest
   * neighbour(s).
   * @param nearestNeighbourSearchAlgorithm - The NearestNeighbourSearch class.
   */
  public void setNearestNeighbourSearchAlgorithm(NearestNeighbourSearch nearestNeighbourSearchAlgorithm) {
<span class="fc" id="L438">    m_NNSearch = nearestNeighbourSearchAlgorithm;</span>
<span class="fc" id="L439">  }</span>
   
  /**
   * Get the number of training instances the classifier is currently using.
   * 
   * @return the number of training instances the classifier is currently using
   */
  public int getNumTraining() {

<span class="nc" id="L448">    return m_Train.numInstances();</span>
  }

  /**
   * Returns default capabilities of the classifier.
   *
   * @return      the capabilities of this classifier
   */
  public Capabilities getCapabilities() {
<span class="fc" id="L457">    Capabilities result = super.getCapabilities();</span>
<span class="fc" id="L458">    result.disableAll();</span>

    // attributes
<span class="fc" id="L461">    result.enable(Capability.NOMINAL_ATTRIBUTES);</span>
<span class="fc" id="L462">    result.enable(Capability.NUMERIC_ATTRIBUTES);</span>
<span class="fc" id="L463">    result.enable(Capability.DATE_ATTRIBUTES);</span>
<span class="fc" id="L464">    result.enable(Capability.MISSING_VALUES);</span>

    // class
<span class="fc" id="L467">    result.enable(Capability.NOMINAL_CLASS);</span>
<span class="fc" id="L468">    result.enable(Capability.NUMERIC_CLASS);</span>
<span class="fc" id="L469">    result.enable(Capability.DATE_CLASS);</span>
<span class="fc" id="L470">    result.enable(Capability.MISSING_CLASS_VALUES);</span>

    // instances
<span class="fc" id="L473">    result.setMinimumNumberInstances(0);</span>
    
<span class="fc" id="L475">    return result;</span>
  }
  
  /**
   * Generates the classifier.
   *
   * @param instances set of instances serving as training data 
   * @throws Exception if the classifier has not been generated successfully
   */
  public void buildClassifier(Instances instances) throws Exception {
    
    // can classifier handle the data?
<span class="fc" id="L487">    getCapabilities().testWithFail(instances);</span>

    // remove instances with missing class
<span class="fc" id="L490">    instances = new Instances(instances);</span>
<span class="fc" id="L491">    instances.deleteWithMissingClass();</span>
    
<span class="fc" id="L493">    m_NumClasses = instances.numClasses();</span>
<span class="fc" id="L494">    m_ClassType = instances.classAttribute().type();</span>
<span class="fc" id="L495">    m_Train = new Instances(instances, 0, instances.numInstances());</span>

    // Throw away initial instances until within the specified window size
<span class="pc bpc" id="L498" title="3 of 4 branches missed.">    if ((m_WindowSize &gt; 0) &amp;&amp; (instances.numInstances() &gt; m_WindowSize)) {</span>
<span class="nc" id="L499">      m_Train = new Instances(m_Train, </span>
<span class="nc" id="L500">			      m_Train.numInstances()-m_WindowSize, </span>
<span class="nc" id="L501">			      m_WindowSize);</span>
    }

<span class="fc" id="L504">    m_NumAttributesUsed = 0.0;</span>
<span class="fc bfc" id="L505" title="All 2 branches covered.">    for (int i = 0; i &lt; m_Train.numAttributes(); i++) {</span>
<span class="fc bfc" id="L506" title="All 2 branches covered.">      if ((i != m_Train.classIndex()) &amp;&amp; </span>
<span class="fc bfc" id="L507" title="All 2 branches covered.">	  (m_Train.attribute(i).isNominal() ||</span>
<span class="pc bpc" id="L508" title="1 of 2 branches missed.">	   m_Train.attribute(i).isNumeric())) {</span>
<span class="fc" id="L509">	m_NumAttributesUsed += 1.0;</span>
      }
    }
    
<span class="fc" id="L513">    m_NNSearch.setInstances(m_Train);</span>

    // Invalidate any currently cross-validation selected k
<span class="fc" id="L516">    m_kNNValid = false;</span>
    
<span class="fc" id="L518">    m_defaultModel = new ZeroR();</span>
<span class="fc" id="L519">    m_defaultModel.buildClassifier(instances);</span>
<span class="fc" id="L520">  }</span>

  /**
   * Adds the supplied instance to the training set.
   *
   * @param instance the instance to add
   * @throws Exception if instance could not be incorporated
   * successfully
   */
  public void updateClassifier(Instance instance) throws Exception {

<span class="pc bpc" id="L531" title="1 of 2 branches missed.">    if (m_Train.equalHeaders(instance.dataset()) == false) {</span>
<span class="nc" id="L532">      throw new Exception(&quot;Incompatible instance types&quot;);</span>
    }
<span class="pc bpc" id="L534" title="1 of 2 branches missed.">    if (instance.classIsMissing()) {</span>
<span class="nc" id="L535">      return;</span>
    }

<span class="fc" id="L538">    m_Train.add(instance);</span>
<span class="fc" id="L539">    m_NNSearch.update(instance);</span>
<span class="fc" id="L540">    m_kNNValid = false;</span>
<span class="pc bpc" id="L541" title="3 of 4 branches missed.">    if ((m_WindowSize &gt; 0) &amp;&amp; (m_Train.numInstances() &gt; m_WindowSize)) {</span>
<span class="nc" id="L542">      boolean deletedInstance=false;</span>
<span class="nc bnc" id="L543" title="All 2 branches missed.">      while (m_Train.numInstances() &gt; m_WindowSize) {</span>
<span class="nc" id="L544">	m_Train.delete(0);</span>
<span class="nc" id="L545">        deletedInstance=true;</span>
      }
      //rebuild datastructure KDTree currently can't delete
<span class="nc bnc" id="L548" title="All 2 branches missed.">      if(deletedInstance==true)</span>
<span class="nc" id="L549">        m_NNSearch.setInstances(m_Train);</span>
    }
<span class="fc" id="L551">  }</span>

  /**
   * Calculates the class membership probabilities for the given test instance.
   *
   * @param instance the instance to be classified
   * @return predicted class probability distribution
   * @throws Exception if an error occurred during the prediction
   */
  public double [] distributionForInstance(Instance instance) throws Exception {

<span class="fc bfc" id="L562" title="All 2 branches covered.">    if (m_Train.numInstances() == 0) {</span>
      //throw new Exception(&quot;No training instances!&quot;);
<span class="fc" id="L564">      return m_defaultModel.distributionForInstance(instance);</span>
    }
<span class="pc bpc" id="L566" title="3 of 4 branches missed.">    if ((m_WindowSize &gt; 0) &amp;&amp; (m_Train.numInstances() &gt; m_WindowSize)) {</span>
<span class="nc" id="L567">      m_kNNValid = false;</span>
<span class="nc" id="L568">      boolean deletedInstance=false;</span>
<span class="nc bnc" id="L569" title="All 2 branches missed.">      while (m_Train.numInstances() &gt; m_WindowSize) {</span>
<span class="nc" id="L570">	m_Train.delete(0);</span>
      }
      //rebuild datastructure KDTree currently can't delete
<span class="nc bnc" id="L573" title="All 2 branches missed.">      if(deletedInstance==true)</span>
<span class="nc" id="L574">        m_NNSearch.setInstances(m_Train);</span>
    }

    // Select k by cross validation
<span class="pc bpc" id="L578" title="4 of 6 branches missed.">    if (!m_kNNValid &amp;&amp; (m_CrossValidate) &amp;&amp; (m_kNNUpper &gt;= 1)) {</span>
<span class="nc" id="L579">      crossValidate();</span>
    }

<span class="fc" id="L582">    m_NNSearch.addInstanceInfo(instance);</span>

<span class="fc" id="L584">    Instances neighbours = m_NNSearch.kNearestNeighbours(instance, m_kNN);</span>
<span class="fc" id="L585">    double [] distances = m_NNSearch.getDistances();</span>
<span class="fc" id="L586">    double [] distribution = makeDistribution( neighbours, distances );</span>

<span class="fc" id="L588">    return distribution;</span>
  }

  /**
   * Returns an enumeration describing the available options.
   *
   * @return an enumeration of all the available options.
   */
  public Enumeration listOptions() {

<span class="fc" id="L598">    Vector newVector = new Vector(8);</span>

<span class="fc" id="L600">    newVector.addElement(new Option(</span>
<span class="fc" id="L601">	      &quot;\tWeight neighbours by the inverse of their distance\n&quot;+</span>
	      &quot;\t(use when k &gt; 1)&quot;,
<span class="fc" id="L603">	      &quot;I&quot;, 0, &quot;-I&quot;));</span>
<span class="fc" id="L604">    newVector.addElement(new Option(</span>
<span class="fc" id="L605">	      &quot;\tWeight neighbours by 1 - their distance\n&quot;+</span>
	      &quot;\t(use when k &gt; 1)&quot;,
<span class="fc" id="L607">	      &quot;F&quot;, 0, &quot;-F&quot;));</span>
<span class="fc" id="L608">    newVector.addElement(new Option(</span>
<span class="fc" id="L609">	      &quot;\tNumber of nearest neighbours (k) used in classification.\n&quot;+</span>
	      &quot;\t(Default = 1)&quot;,
<span class="fc" id="L611">	      &quot;K&quot;, 1,&quot;-K &lt;number of neighbors&gt;&quot;));</span>
<span class="fc" id="L612">    newVector.addElement(new Option(</span>
<span class="fc" id="L613">          &quot;\tMinimise mean squared error rather than mean absolute\n&quot;+</span>
	      &quot;\terror when using -X option with numeric prediction.&quot;,
<span class="fc" id="L615">	      &quot;E&quot;, 0,&quot;-E&quot;));</span>
<span class="fc" id="L616">    newVector.addElement(new Option(</span>
<span class="fc" id="L617">          &quot;\tMaximum number of training instances maintained.\n&quot;+</span>
	      &quot;\tTraining instances are dropped FIFO. (Default = no window)&quot;,
<span class="fc" id="L619">	      &quot;W&quot;, 1,&quot;-W &lt;window size&gt;&quot;));</span>
<span class="fc" id="L620">    newVector.addElement(new Option(</span>
<span class="fc" id="L621">	      &quot;\tSelect the number of nearest neighbours between 1\n&quot;+</span>
	      &quot;\tand the k value specified using hold-one-out evaluation\n&quot;+
	      &quot;\ton the training data (use when k &gt; 1)&quot;,
<span class="fc" id="L624">	      &quot;X&quot;, 0,&quot;-X&quot;));</span>
<span class="fc" id="L625">    newVector.addElement(new Option(</span>
<span class="fc" id="L626">	      &quot;\tThe nearest neighbour search algorithm to use &quot;+</span>
          &quot;(default: weka.core.neighboursearch.LinearNNSearch).\n&quot;,
<span class="fc" id="L628">	      &quot;A&quot;, 0, &quot;-A&quot;));</span>

<span class="fc" id="L630">    return newVector.elements();</span>
  }

  /**
   * Parses a given list of options. &lt;p/&gt;
   *
   &lt;!-- options-start --&gt;
   * Valid options are: &lt;p/&gt;
   * 
   * &lt;pre&gt; -I
   *  Weight neighbours by the inverse of their distance
   *  (use when k &amp;gt; 1)&lt;/pre&gt;
   * 
   * &lt;pre&gt; -F
   *  Weight neighbours by 1 - their distance
   *  (use when k &amp;gt; 1)&lt;/pre&gt;
   * 
   * &lt;pre&gt; -K &amp;lt;number of neighbors&amp;gt;
   *  Number of nearest neighbours (k) used in classification.
   *  (Default = 1)&lt;/pre&gt;
   * 
   * &lt;pre&gt; -E
   *  Minimise mean squared error rather than mean absolute
   *  error when using -X option with numeric prediction.&lt;/pre&gt;
   * 
   * &lt;pre&gt; -W &amp;lt;window size&amp;gt;
   *  Maximum number of training instances maintained.
   *  Training instances are dropped FIFO. (Default = no window)&lt;/pre&gt;
   * 
   * &lt;pre&gt; -X
   *  Select the number of nearest neighbours between 1
   *  and the k value specified using hold-one-out evaluation
   *  on the training data (use when k &amp;gt; 1)&lt;/pre&gt;
   * 
   * &lt;pre&gt; -A
   *  The nearest neighbour search algorithm to use (default: weka.core.neighboursearch.LinearNNSearch).
   * &lt;/pre&gt;
   * 
   &lt;!-- options-end --&gt;
   *
   * @param options the list of options as an array of strings
   * @throws Exception if an option is not supported
   */
  public void setOptions(String[] options) throws Exception {
    
<span class="fc" id="L675">    String knnString = Utils.getOption('K', options);</span>
<span class="fc bfc" id="L676" title="All 2 branches covered.">    if (knnString.length() != 0) {</span>
<span class="fc" id="L677">      setKNN(Integer.parseInt(knnString));</span>
    } else {
<span class="fc" id="L679">      setKNN(1);</span>
    }
<span class="fc" id="L681">    String windowString = Utils.getOption('W', options);</span>
<span class="fc bfc" id="L682" title="All 2 branches covered.">    if (windowString.length() != 0) {</span>
<span class="fc" id="L683">      setWindowSize(Integer.parseInt(windowString));</span>
    } else {
<span class="fc" id="L685">      setWindowSize(0);</span>
    }
<span class="pc bpc" id="L687" title="1 of 2 branches missed.">    if (Utils.getFlag('I', options)) {</span>
<span class="nc" id="L688">      setDistanceWeighting(new SelectedTag(WEIGHT_INVERSE, TAGS_WEIGHTING));</span>
<span class="pc bpc" id="L689" title="1 of 2 branches missed.">    } else if (Utils.getFlag('F', options)) {</span>
<span class="nc" id="L690">      setDistanceWeighting(new SelectedTag(WEIGHT_SIMILARITY, TAGS_WEIGHTING));</span>
    } else {
<span class="fc" id="L692">      setDistanceWeighting(new SelectedTag(WEIGHT_NONE, TAGS_WEIGHTING));</span>
    }
<span class="fc" id="L694">    setCrossValidate(Utils.getFlag('X', options));</span>
<span class="fc" id="L695">    setMeanSquared(Utils.getFlag('E', options));</span>

<span class="fc" id="L697">    String nnSearchClass = Utils.getOption('A', options);</span>
<span class="fc bfc" id="L698" title="All 2 branches covered.">    if(nnSearchClass.length() != 0) {</span>
<span class="fc" id="L699">      String nnSearchClassSpec[] = Utils.splitOptions(nnSearchClass);</span>
<span class="pc bpc" id="L700" title="1 of 2 branches missed.">      if(nnSearchClassSpec.length == 0) { </span>
<span class="nc" id="L701">        throw new Exception(&quot;Invalid NearestNeighbourSearch algorithm &quot; +</span>
                            &quot;specification string.&quot;); 
      }
<span class="fc" id="L704">      String className = nnSearchClassSpec[0];</span>
<span class="fc" id="L705">      nnSearchClassSpec[0] = &quot;&quot;;</span>

<span class="fc" id="L707">      setNearestNeighbourSearchAlgorithm( (NearestNeighbourSearch)</span>
<span class="fc" id="L708">                  Utils.forName( NearestNeighbourSearch.class, </span>
<span class="fc" id="L709">                                 className, </span>
<span class="fc" id="L710">                                 nnSearchClassSpec)</span>
                                        );
    }
    else 
<span class="fc" id="L714">      this.setNearestNeighbourSearchAlgorithm(new LinearNNSearch());</span>
    
<span class="fc" id="L716">    Utils.checkForRemainingOptions(options);</span>
<span class="fc" id="L717">  }</span>

  /**
   * Gets the current settings of IBk.
   *
   * @return an array of strings suitable for passing to setOptions()
   */
  public String [] getOptions() {

<span class="fc" id="L726">    String [] options = new String [11];</span>
<span class="fc" id="L727">    int current = 0;</span>
<span class="fc" id="L728">    options[current++] = &quot;-K&quot;; options[current++] = &quot;&quot; + getKNN();</span>
<span class="fc" id="L729">    options[current++] = &quot;-W&quot;; options[current++] = &quot;&quot; + m_WindowSize;</span>
<span class="pc bpc" id="L730" title="1 of 2 branches missed.">    if (getCrossValidate()) {</span>
<span class="nc" id="L731">      options[current++] = &quot;-X&quot;;</span>
    }
<span class="pc bpc" id="L733" title="1 of 2 branches missed.">    if (getMeanSquared()) {</span>
<span class="nc" id="L734">      options[current++] = &quot;-E&quot;;</span>
    }
<span class="pc bpc" id="L736" title="1 of 2 branches missed.">    if (m_DistanceWeighting == WEIGHT_INVERSE) {</span>
<span class="nc" id="L737">      options[current++] = &quot;-I&quot;;</span>
<span class="pc bpc" id="L738" title="1 of 2 branches missed.">    } else if (m_DistanceWeighting == WEIGHT_SIMILARITY) {</span>
<span class="nc" id="L739">      options[current++] = &quot;-F&quot;;</span>
    }

<span class="fc" id="L742">    options[current++] = &quot;-A&quot;;</span>
<span class="fc" id="L743">    options[current++] = m_NNSearch.getClass().getName()+&quot; &quot;+Utils.joinOptions(m_NNSearch.getOptions()); </span>
    
<span class="fc bfc" id="L745" title="All 2 branches covered.">    while (current &lt; options.length) {</span>
<span class="fc" id="L746">      options[current++] = &quot;&quot;;</span>
    }
    
<span class="fc" id="L749">    return options;</span>
  }

  /**
   * Returns an enumeration of the additional measure names 
   * produced by the neighbour search algorithm, plus the chosen K in case
   * cross-validation is enabled.
   * 
   * @return an enumeration of the measure names
   */
  public Enumeration enumerateMeasures() {
<span class="nc bnc" id="L760" title="All 2 branches missed.">    if (m_CrossValidate) {</span>
<span class="nc" id="L761">      Enumeration enm = m_NNSearch.enumerateMeasures();</span>
<span class="nc" id="L762">      Vector measures = new Vector();</span>
<span class="nc bnc" id="L763" title="All 2 branches missed.">      while (enm.hasMoreElements())</span>
<span class="nc" id="L764">	measures.add(enm.nextElement());</span>
<span class="nc" id="L765">      measures.add(&quot;measureKNN&quot;);</span>
<span class="nc" id="L766">      return measures.elements();</span>
    }
    else {
<span class="nc" id="L769">      return m_NNSearch.enumerateMeasures();</span>
    }
  }
  
  /**
   * Returns the value of the named measure from the 
   * neighbour search algorithm, plus the chosen K in case
   * cross-validation is enabled.
   * 
   * @param additionalMeasureName the name of the measure to query for its value
   * @return the value of the named measure
   * @throws IllegalArgumentException if the named measure is not supported
   */
  public double getMeasure(String additionalMeasureName) {
<span class="nc bnc" id="L783" title="All 2 branches missed.">    if (additionalMeasureName.equals(&quot;measureKNN&quot;))</span>
<span class="nc" id="L784">      return m_kNN;</span>
    else
<span class="nc" id="L786">      return m_NNSearch.getMeasure(additionalMeasureName);</span>
  }
  
  
  /**
   * Returns a description of this classifier.
   *
   * @return a description of this classifier as a string.
   */
  public String toString() {

<span class="pc bpc" id="L797" title="1 of 2 branches missed.">    if (m_Train == null) {</span>
<span class="fc" id="L798">      return &quot;IBk: No model built yet.&quot;;</span>
    }
    
<span class="nc bnc" id="L801" title="All 2 branches missed.">    if (m_Train.numInstances() == 0) {</span>
<span class="nc" id="L802">      return &quot;Warning: no training instances - ZeroR model used.&quot;;</span>
    }    

<span class="nc bnc" id="L805" title="All 4 branches missed.">    if (!m_kNNValid &amp;&amp; m_CrossValidate) {</span>
<span class="nc" id="L806">      crossValidate();</span>
    }
    
    

<span class="nc" id="L811">    String result = &quot;IB1 instance-based classifier\n&quot; +</span>
<span class="nc" id="L812">      &quot;using &quot; + m_kNN;</span>

<span class="nc bnc" id="L814" title="All 3 branches missed.">    switch (m_DistanceWeighting) {</span>
    case WEIGHT_INVERSE:
<span class="nc" id="L816">      result += &quot; inverse-distance-weighted&quot;;</span>
<span class="nc" id="L817">      break;</span>
    case WEIGHT_SIMILARITY:
<span class="nc" id="L819">      result += &quot; similarity-weighted&quot;;</span>
      break;
<span class="nc" id="L821">    }</span>
<span class="nc" id="L822">    result += &quot; nearest neighbour(s) for classification\n&quot;;</span>

<span class="nc bnc" id="L824" title="All 2 branches missed.">    if (m_WindowSize != 0) {</span>
<span class="nc" id="L825">      result += &quot;using a maximum of &quot; </span>
<span class="nc" id="L826">	+ m_WindowSize + &quot; (windowed) training instances\n&quot;;</span>
    }
<span class="nc" id="L828">    return result;</span>
  }

  /**
   * Initialise scheme variables.
   */
  protected void init() {

<span class="fc" id="L836">    setKNN(1);</span>
<span class="fc" id="L837">    m_WindowSize = 0;</span>
<span class="fc" id="L838">    m_DistanceWeighting = WEIGHT_NONE;</span>
<span class="fc" id="L839">    m_CrossValidate = false;</span>
<span class="fc" id="L840">    m_MeanSquared = false;</span>
<span class="fc" id="L841">  }</span>
  
  /**
   * Turn the list of nearest neighbors into a probability distribution.
   *
   * @param neighbours the list of nearest neighboring instances
   * @param distances the distances of the neighbors
   * @return the probability distribution
   * @throws Exception if computation goes wrong or has no class attribute
   */
  protected double [] makeDistribution(Instances neighbours, double[] distances)
    throws Exception {

<span class="fc" id="L854">    double total = 0, weight;</span>
<span class="fc" id="L855">    double [] distribution = new double [m_NumClasses];</span>
    
    // Set up a correction to the estimator
<span class="fc bfc" id="L858" title="All 2 branches covered.">    if (m_ClassType == Attribute.NOMINAL) {</span>
<span class="fc bfc" id="L859" title="All 2 branches covered.">      for(int i = 0; i &lt; m_NumClasses; i++) {</span>
<span class="fc" id="L860">	distribution[i] = 1.0 / Math.max(1,m_Train.numInstances());</span>
      }
<span class="fc" id="L862">      total = (double)m_NumClasses / Math.max(1,m_Train.numInstances());</span>
    }

<span class="fc bfc" id="L865" title="All 2 branches covered.">    for(int i=0; i &lt; neighbours.numInstances(); i++) {</span>
      // Collect class counts
<span class="fc" id="L867">      Instance current = neighbours.instance(i);</span>
<span class="fc" id="L868">      distances[i] = distances[i]*distances[i];</span>
<span class="fc" id="L869">      distances[i] = Math.sqrt(distances[i]/m_NumAttributesUsed);</span>
<span class="pc bpc" id="L870" title="2 of 3 branches missed.">      switch (m_DistanceWeighting) {</span>
        case WEIGHT_INVERSE:
<span class="nc" id="L872">          weight = 1.0 / (distances[i] + 0.001); // to avoid div by zero</span>
<span class="nc" id="L873">          break;</span>
        case WEIGHT_SIMILARITY:
<span class="nc" id="L875">          weight = 1.0 - distances[i];</span>
<span class="nc" id="L876">          break;</span>
        default:                                 // WEIGHT_NONE:
<span class="fc" id="L878">          weight = 1.0;</span>
<span class="fc" id="L879">          break;</span>
      }
<span class="fc" id="L881">      weight *= current.weight();</span>
      try {
<span class="fc bfc" id="L883" title="All 3 branches covered.">        switch (m_ClassType) {</span>
          case Attribute.NOMINAL:
<span class="fc" id="L885">            distribution[(int)current.classValue()] += weight;</span>
<span class="fc" id="L886">            break;</span>
          case Attribute.NUMERIC:
<span class="fc" id="L888">            distribution[0] += current.classValue() * weight;</span>
            break;
<span class="fc" id="L890">        }</span>
<span class="nc" id="L891">      } catch (Exception ex) {</span>
<span class="nc" id="L892">        throw new Error(&quot;Data has no class attribute!&quot;);</span>
      }
<span class="fc" id="L894">      total += weight;      </span>
    }

    // Normalise distribution
<span class="fc bfc" id="L898" title="All 2 branches covered.">    if (total &gt; 0) {</span>
<span class="fc" id="L899">      Utils.normalize(distribution, total);</span>
    }
<span class="fc" id="L901">    return distribution;</span>
  }

  /**
   * Select the best value for k by hold-one-out cross-validation.
   * If the class attribute is nominal, classification error is
   * minimised. If the class attribute is numeric, mean absolute
   * error is minimised
   */
  protected void crossValidate() {

    try {
<span class="nc bnc" id="L913" title="All 2 branches missed.">      if (m_NNSearch instanceof weka.core.neighboursearch.CoverTree)</span>
<span class="nc" id="L914">	throw new Exception(&quot;CoverTree doesn't support hold-one-out &quot;+</span>
			    &quot;cross-validation. Use some other NN &quot; +
			    &quot;method.&quot;);

<span class="nc" id="L918">      double [] performanceStats = new double [m_kNNUpper];</span>
<span class="nc" id="L919">      double [] performanceStatsSq = new double [m_kNNUpper];</span>

<span class="nc bnc" id="L921" title="All 2 branches missed.">      for(int i = 0; i &lt; m_kNNUpper; i++) {</span>
<span class="nc" id="L922">	performanceStats[i] = 0;</span>
<span class="nc" id="L923">	performanceStatsSq[i] = 0;</span>
      }


<span class="nc" id="L927">      m_kNN = m_kNNUpper;</span>
      Instance instance;
      Instances neighbours;
      double[] origDistances, convertedDistances;
<span class="nc bnc" id="L931" title="All 2 branches missed.">      for(int i = 0; i &lt; m_Train.numInstances(); i++) {</span>
<span class="nc bnc" id="L932" title="All 4 branches missed.">	if (m_Debug &amp;&amp; (i % 50 == 0)) {</span>
<span class="nc" id="L933">	  System.err.print(&quot;Cross validating &quot;</span>
<span class="nc" id="L934">			   + i + &quot;/&quot; + m_Train.numInstances() + &quot;\r&quot;);</span>
	}
<span class="nc" id="L936">	instance = m_Train.instance(i);</span>
<span class="nc" id="L937">	neighbours = m_NNSearch.kNearestNeighbours(instance, m_kNN);</span>
<span class="nc" id="L938">        origDistances = m_NNSearch.getDistances();</span>
        
<span class="nc bnc" id="L940" title="All 2 branches missed.">	for(int j = m_kNNUpper - 1; j &gt;= 0; j--) {</span>
	  // Update the performance stats
<span class="nc" id="L942">          convertedDistances = new double[origDistances.length];</span>
<span class="nc" id="L943">          System.arraycopy(origDistances, 0, </span>
<span class="nc" id="L944">                           convertedDistances, 0, origDistances.length);</span>
<span class="nc" id="L945">	  double [] distribution = makeDistribution(neighbours, </span>
<span class="nc" id="L946">                                                    convertedDistances);</span>
<span class="nc" id="L947">          double thisPrediction = Utils.maxIndex(distribution);</span>
<span class="nc bnc" id="L948" title="All 2 branches missed.">	  if (m_Train.classAttribute().isNumeric()) {</span>
<span class="nc" id="L949">	    thisPrediction = distribution[0];</span>
<span class="nc" id="L950">	    double err = thisPrediction - instance.classValue();</span>
<span class="nc" id="L951">	    performanceStatsSq[j] += err * err;   // Squared error</span>
<span class="nc" id="L952">	    performanceStats[j] += Math.abs(err); // Absolute error</span>
	  } else {
<span class="nc bnc" id="L954" title="All 2 branches missed.">	    if (thisPrediction != instance.classValue()) {</span>
<span class="nc" id="L955">	      performanceStats[j] ++;             // Classification error</span>
	    }
	  }
<span class="nc bnc" id="L958" title="All 2 branches missed.">	  if (j &gt;= 1) {</span>
<span class="nc" id="L959">	    neighbours = pruneToK(neighbours, convertedDistances, j);</span>
	  }
	}
      }

      // Display the results of the cross-validation
<span class="nc bnc" id="L965" title="All 2 branches missed.">      for(int i = 0; i &lt; m_kNNUpper; i++) {</span>
<span class="nc bnc" id="L966" title="All 2 branches missed.">	if (m_Debug) {</span>
<span class="nc" id="L967">	  System.err.print(&quot;Hold-one-out performance of &quot; + (i + 1)</span>
<span class="nc" id="L968">			   + &quot; neighbors &quot; );</span>
	}
<span class="nc bnc" id="L970" title="All 2 branches missed.">	if (m_Train.classAttribute().isNumeric()) {</span>
<span class="nc bnc" id="L971" title="All 2 branches missed.">	  if (m_Debug) {</span>
<span class="nc bnc" id="L972" title="All 2 branches missed.">	    if (m_MeanSquared) {</span>
<span class="nc" id="L973">	      System.err.println(&quot;(RMSE) = &quot;</span>
<span class="nc" id="L974">				 + Math.sqrt(performanceStatsSq[i]</span>
<span class="nc" id="L975">					     / m_Train.numInstances()));</span>
	    } else {
<span class="nc" id="L977">	      System.err.println(&quot;(MAE) = &quot;</span>
<span class="nc" id="L978">				 + performanceStats[i]</span>
<span class="nc" id="L979">				 / m_Train.numInstances());</span>
	    }
	  }
	} else {
<span class="nc bnc" id="L983" title="All 2 branches missed.">	  if (m_Debug) {</span>
<span class="nc" id="L984">	    System.err.println(&quot;(%ERR) = &quot;</span>
<span class="nc" id="L985">			       + 100.0 * performanceStats[i]</span>
<span class="nc" id="L986">			       / m_Train.numInstances());</span>
	  }
	}
      }


      // Check through the performance stats and select the best
      // k value (or the lowest k if more than one best)
<span class="nc" id="L994">      double [] searchStats = performanceStats;</span>
<span class="nc bnc" id="L995" title="All 4 branches missed.">      if (m_Train.classAttribute().isNumeric() &amp;&amp; m_MeanSquared) {</span>
<span class="nc" id="L996">	searchStats = performanceStatsSq;</span>
      }
<span class="nc" id="L998">      double bestPerformance = Double.NaN;</span>
<span class="nc" id="L999">      int bestK = 1;</span>
<span class="nc bnc" id="L1000" title="All 2 branches missed.">      for(int i = 0; i &lt; m_kNNUpper; i++) {</span>
<span class="nc bnc" id="L1001" title="All 2 branches missed.">	if (Double.isNaN(bestPerformance)</span>
<span class="nc bnc" id="L1002" title="All 2 branches missed.">	    || (bestPerformance &gt; searchStats[i])) {</span>
<span class="nc" id="L1003">	  bestPerformance = searchStats[i];</span>
<span class="nc" id="L1004">	  bestK = i + 1;</span>
	}
      }
<span class="nc" id="L1007">      m_kNN = bestK;</span>
<span class="nc bnc" id="L1008" title="All 2 branches missed.">      if (m_Debug) {</span>
<span class="nc" id="L1009">	System.err.println(&quot;Selected k = &quot; + bestK);</span>
      }
      
<span class="nc" id="L1012">      m_kNNValid = true;</span>
<span class="nc" id="L1013">    } catch (Exception ex) {</span>
<span class="nc" id="L1014">      throw new Error(&quot;Couldn't optimize by cross-validation: &quot;</span>
<span class="nc" id="L1015">		      +ex.getMessage());</span>
    }
<span class="nc" id="L1017">  }</span>
  
  /**
   * Prunes the list to contain the k nearest neighbors. If there are
   * multiple neighbors at the k'th distance, all will be kept.
   *
   * @param neighbours the neighbour instances.
   * @param distances the distances of the neighbours from target instance.
   * @param k the number of neighbors to keep.
   * @return the pruned neighbours.
   */
  public Instances pruneToK(Instances neighbours, double[] distances, int k) {
    
<span class="nc bnc" id="L1030" title="All 6 branches missed.">    if(neighbours==null || distances==null || neighbours.numInstances()==0) {</span>
<span class="nc" id="L1031">      return null;</span>
    }
<span class="nc bnc" id="L1033" title="All 2 branches missed.">    if (k &lt; 1) {</span>
<span class="nc" id="L1034">      k = 1;</span>
    }
    
<span class="nc" id="L1037">    int currentK = 0;</span>
    double currentDist;
<span class="nc bnc" id="L1039" title="All 2 branches missed.">    for(int i=0; i &lt; neighbours.numInstances(); i++) {</span>
<span class="nc" id="L1040">      currentK++;</span>
<span class="nc" id="L1041">      currentDist = distances[i];</span>
<span class="nc bnc" id="L1042" title="All 4 branches missed.">      if(currentK&gt;k &amp;&amp; currentDist!=distances[i-1]) {</span>
<span class="nc" id="L1043">        currentK--;</span>
<span class="nc" id="L1044">        neighbours = new Instances(neighbours, 0, currentK);</span>
<span class="nc" id="L1045">        break;</span>
      }
    }

<span class="nc" id="L1049">    return neighbours;</span>
  }
  
  /**
   * Returns the revision string.
   * 
   * @return		the revision
   */
  public String getRevision() {
<span class="nc" id="L1058">    return RevisionUtils.extract(&quot;$Revision: 6573 $&quot;);</span>
  }
  
  /**
   * Main method for testing this class.
   *
   * @param argv should contain command line options (see setOptions)
   */
  public static void main(String [] argv) {
<span class="nc" id="L1067">    runClassifier(new IBk(), argv);</span>
<span class="nc" id="L1068">  }</span>
}
</pre><div class="footer"><span class="right">Created with <a href="http://www.eclemma.org/jacoco">JaCoCo</a> 0.7.2.201409121644</span>AllTests (Nov 28, 2015 2:34:31 PM)</div></body></html>
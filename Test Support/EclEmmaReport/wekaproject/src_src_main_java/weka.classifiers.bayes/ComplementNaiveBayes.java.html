<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><meta http-equiv="Content-Type" content="text/html;charset=UTF-8"/><link rel="stylesheet" href="../../../.resources/report.css" type="text/css"/><link rel="shortcut icon" href="../../../.resources/report.gif" type="image/gif"/><title>ComplementNaiveBayes.java</title><link rel="stylesheet" href="../../../.resources/prettify.css" type="text/css"/><script type="text/javascript" src="../../../.resources/prettify.js"></script></head><body onload="window['PR_TAB_WIDTH']=4;prettyPrint()"><div class="breadcrumb" id="breadcrumb"><span class="info"><a href="../../../.sessions.html" class="el_session">Sessions</a></span><a href="../../../index.html" class="el_report">AllTests (Nov 28, 2015 2:34:31 PM)</a> &gt; <a href="../../index.html" class="el_group">wekaproject</a> &gt; <a href="../index.html" class="el_bundle">src/src/main/java</a> &gt; <a href="index.source.html" class="el_package">weka.classifiers.bayes</a> &gt; <span class="el_source">ComplementNaiveBayes.java</span></div><h1>ComplementNaiveBayes.java</h1><pre class="source lang-java linenums">/*
 *    This program is free software; you can redistribute it and/or modify
 *    it under the terms of the GNU General Public License as published by
 *    the Free Software Foundation; either version 2 of the License, or
 *    (at your option) any later version.
 *
 *    This program is distributed in the hope that it will be useful,
 *    but WITHOUT ANY WARRANTY; without even the implied warranty of
 *    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 *    GNU General Public License for more details.
 *
 *    You should have received a copy of the GNU General Public License
 *    along with this program; if not, write to the Free Software
 *    Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
 */

/*
 *    ComplementNaiveBayes.java
 *    Copyright (C) 2003 University of Waikato, Hamilton, New Zealand
 */

package weka.classifiers.bayes;

import weka.classifiers.Classifier;
import weka.core.Capabilities;
import weka.core.FastVector;
import weka.core.Instance;
import weka.core.Instances;
import weka.core.Option;
import weka.core.OptionHandler;
import weka.core.RevisionUtils;
import weka.core.TechnicalInformation;
import weka.core.TechnicalInformationHandler;
import weka.core.Utils;
import weka.core.WeightedInstancesHandler;
import weka.core.Capabilities.Capability;
import weka.core.TechnicalInformation.Field;
import weka.core.TechnicalInformation.Type;


/**
 &lt;!-- globalinfo-start --&gt;
 * Class for building and using a Complement class Naive Bayes classifier.&lt;br/&gt;
 * &lt;br/&gt;
 * For more information see, &lt;br/&gt;
 * &lt;br/&gt;
 * Jason D. Rennie, Lawrence Shih, Jaime Teevan, David R. Karger: Tackling the Poor Assumptions of Naive Bayes Text Classifiers. In: ICML, 616-623, 2003.&lt;br/&gt;
 * &lt;br/&gt;
 * P.S.: TF, IDF and length normalization transforms, as described in the paper, can be performed through weka.filters.unsupervised.StringToWordVector.
 * &lt;p/&gt;
 &lt;!-- globalinfo-end --&gt;
 *
 &lt;!-- technical-bibtex-start --&gt;
 * BibTeX:
 * &lt;pre&gt;
 * &amp;#64;inproceedings{Rennie2003,
 *    author = {Jason D. Rennie and Lawrence Shih and Jaime Teevan and David R. Karger},
 *    booktitle = {ICML},
 *    pages = {616-623},
 *    publisher = {AAAI Press},
 *    title = {Tackling the Poor Assumptions of Naive Bayes Text Classifiers},
 *    year = {2003}
 * }
 * &lt;/pre&gt;
 * &lt;p/&gt;
 &lt;!-- technical-bibtex-end --&gt;
 *
 &lt;!-- options-start --&gt;
 * Valid options are: &lt;p/&gt;
 * 
 * &lt;pre&gt; -N
 *  Normalize the word weights for each class
 * &lt;/pre&gt;
 * 
 * &lt;pre&gt; -S
 *  Smoothing value to avoid zero WordGivenClass probabilities (default=1.0).
 * &lt;/pre&gt;
 * 
 &lt;!-- options-end --&gt;
 *
 * @author Ashraf M. Kibriya (amk14@cs.waikato.ac.nz)
 * @version $Revision: 5516 $ 
 */
<span class="fc" id="L84">public class ComplementNaiveBayes extends Classifier</span>
    implements OptionHandler, WeightedInstancesHandler, TechnicalInformationHandler {
    
    /** for serialization */
    static final long serialVersionUID = 7246302925903086397L;
  
    /**
      Weight of words for each class. The weight is actually the
      log of the probability of a word (w) given a class (c) 
      (i.e. log(Pr[w|c])). The format of the matrix is: 
      wordWeights[class][wordAttribute]
    */
    private double[][] wordWeights;
    
    /** Holds the smoothing value to avoid word probabilities of zero.&lt;br&gt;
        P.S.: According to the paper this is the Alpha i parameter 
     */
<span class="fc" id="L101">    private double smoothingParameter = 1.0;</span>
    
    /** True if the words weights are to be normalized */
<span class="fc" id="L104">    private boolean m_normalizeWordWeights = false;</span>
    
    /** Holds the number of Class values present in the set of specified 
        instances */
    private int numClasses;
    
    /** The instances header that'll be used in toString */
    private Instances header;

    
    /**
     * Returns an enumeration describing the available options.
     *
     * @return an enumeration of all the available options.
     */
    public java.util.Enumeration listOptions() {
<span class="fc" id="L120">        FastVector newVector = new FastVector(2);</span>
<span class="fc" id="L121">        newVector.addElement(</span>
<span class="fc" id="L122">        new Option(&quot;\tNormalize the word weights for each class\n&quot;,</span>
<span class="fc" id="L123">                   &quot;N&quot;, 0,&quot;-N&quot;));</span>
<span class="fc" id="L124">        newVector.addElement(</span>
<span class="fc" id="L125">        new Option(&quot;\tSmoothing value to avoid zero WordGivenClass&quot;+</span>
                   &quot; probabilities (default=1.0).\n&quot;,
<span class="fc" id="L127">                   &quot;S&quot;, 1,&quot;-S&quot;));</span>
        
<span class="fc" id="L129">        return newVector.elements();</span>
    }
    
    /**
     * Gets the current settings of the classifier.
     *
     * @return an array of strings suitable for passing to setOptions
     */
    public String[] getOptions() {
<span class="fc" id="L138">        String options[] = new String[4];</span>
<span class="fc" id="L139">        int current=0;</span>
        
<span class="pc bpc" id="L141" title="1 of 2 branches missed.">        if(getNormalizeWordWeights())</span>
<span class="nc" id="L142">            options[current++] = &quot;-N&quot;;</span>
        
<span class="fc" id="L144">        options[current++] = &quot;-S&quot;;</span>
<span class="fc" id="L145">        options[current++] = Double.toString(smoothingParameter);</span>
        
<span class="fc bfc" id="L147" title="All 2 branches covered.">        while (current &lt; options.length) {</span>
<span class="fc" id="L148">            options[current++] = &quot;&quot;;</span>
        }
        
<span class="fc" id="L151">        return options;</span>
    }        

    /**
     * Parses a given list of options. &lt;p/&gt;
     *
     &lt;!-- options-start --&gt;
     * Valid options are: &lt;p/&gt;
     * 
     * &lt;pre&gt; -N
     *  Normalize the word weights for each class
     * &lt;/pre&gt;
     * 
     * &lt;pre&gt; -S
     *  Smoothing value to avoid zero WordGivenClass probabilities (default=1.0).
     * &lt;/pre&gt;
     * 
     &lt;!-- options-end --&gt;
     *
     * @param options the list of options as an array of strings
     * @throws Exception if an option is not supported
     */
    public void setOptions(String[] options) throws Exception {
        
<span class="fc" id="L175">        setNormalizeWordWeights(Utils.getFlag('N', options));</span>
        
<span class="fc" id="L177">        String val = Utils.getOption('S', options);</span>
<span class="fc bfc" id="L178" title="All 2 branches covered.">        if(val.length()!=0)</span>
<span class="fc" id="L179">          setSmoothingParameter(Double.parseDouble(val));</span>
        else
<span class="fc" id="L181">          setSmoothingParameter(1.0);</span>
<span class="fc" id="L182">    }</span>
    
    /**
     * Returns true if the word weights for each class are to be normalized
     * 
     * @return true if the word weights are normalized
     */
    public boolean getNormalizeWordWeights() {
<span class="fc" id="L190">        return m_normalizeWordWeights;</span>
    }
    
    /**
     * Sets whether if the word weights for each class should be normalized
     * 
     * @param doNormalize whether the word weights are to be normalized
     */
    public void setNormalizeWordWeights(boolean doNormalize) {
<span class="fc" id="L199">        m_normalizeWordWeights = doNormalize;</span>
<span class="fc" id="L200">    }</span>
    
    /**
     * Returns the tip text for this property
     * @return tip text for this property suitable for
     * displaying in the explorer/experimenter gui
     */
    public String normalizeWordWeightsTipText() {
<span class="nc" id="L208">        return &quot;Normalizes the word weights for each class.&quot;;</span>
    }
    
    /**
     * Gets the smoothing value to be used to avoid zero WordGivenClass
     * probabilities.
     * 
     * @return the smoothing value
     */
    public double getSmoothingParameter() {
<span class="nc" id="L218">        return smoothingParameter;</span>
    }

    /**
     * Sets the smoothing value used to avoid zero WordGivenClass probabilities
     * 
     * @param val the new smooting value
     */
    public void setSmoothingParameter(double val) {
<span class="fc" id="L227">        smoothingParameter = val;</span>
<span class="fc" id="L228">    }</span>
        
    /**
     * Returns the tip text for this property
     * @return tip text for this property suitable for
     * displaying in the explorer/experimenter gui
     */
    public String smoothingParameterTipText() {
<span class="nc" id="L236">        return &quot;Sets the smoothing parameter to avoid zero WordGivenClass &quot;+</span>
               &quot;probabilities (default=1.0).&quot;;
    }

    /**
     * Returns a string describing this classifier
     * @return a description of the classifier suitable for
     * displaying in the explorer/experimenter gui
     */
    public String globalInfo() {
        
<span class="nc" id="L247">        return  &quot;Class for building and using a Complement class Naive Bayes &quot;+</span>
                &quot;classifier.\n\nFor more information see, \n\n&quot;+
<span class="nc" id="L249">                getTechnicalInformation().toString() + &quot;\n\n&quot; +</span>
<span class="nc" id="L250">                &quot;P.S.: TF, IDF and length normalization transforms, as &quot;+</span>
<span class="nc" id="L251">                &quot;described in the paper, can be performed through &quot;+</span>
<span class="nc" id="L252">                &quot;weka.filters.unsupervised.StringToWordVector.&quot;;</span>
    }

    /**
     * Returns an instance of a TechnicalInformation object, containing 
     * detailed information about the technical background of this class,
     * e.g., paper reference or book this class is based on.
     * 
     * @return the technical information about this class
     */
    public TechnicalInformation getTechnicalInformation() {
      TechnicalInformation 	result;
      
<span class="nc" id="L265">      result = new TechnicalInformation(Type.INPROCEEDINGS);</span>
<span class="nc" id="L266">      result.setValue(Field.AUTHOR, &quot;Jason D. Rennie and Lawrence Shih and Jaime Teevan and David R. Karger&quot;);</span>
<span class="nc" id="L267">      result.setValue(Field.TITLE, &quot;Tackling the Poor Assumptions of Naive Bayes Text Classifiers&quot;);</span>
<span class="nc" id="L268">      result.setValue(Field.BOOKTITLE, &quot;ICML&quot;);</span>
<span class="nc" id="L269">      result.setValue(Field.YEAR, &quot;2003&quot;);</span>
<span class="nc" id="L270">      result.setValue(Field.PAGES, &quot;616-623&quot;);</span>
<span class="nc" id="L271">      result.setValue(Field.PUBLISHER, &quot;AAAI Press&quot;);</span>
      
<span class="nc" id="L273">      return result;</span>
    }

    /**
     * Returns default capabilities of the classifier.
     *
     * @return      the capabilities of this classifier
     */
    public Capabilities getCapabilities() {
<span class="fc" id="L282">      Capabilities result = super.getCapabilities();</span>
<span class="fc" id="L283">      result.disableAll();</span>

      // attributes
<span class="fc" id="L286">      result.enable(Capability.NUMERIC_ATTRIBUTES);</span>
<span class="fc" id="L287">      result.enable(Capability.MISSING_VALUES);</span>

      // class
<span class="fc" id="L290">      result.enable(Capability.NOMINAL_CLASS);</span>
<span class="fc" id="L291">      result.enable(Capability.MISSING_CLASS_VALUES);</span>
      
<span class="fc" id="L293">      return result;</span>
    }
    
    /**
     * Generates the classifier.
     *
     * @param instances set of instances serving as training data 
     * @throws Exception if the classifier has not been built successfully
     */
    public void buildClassifier(Instances instances) throws Exception {

      // can classifier handle the data?
<span class="fc" id="L305">      getCapabilities().testWithFail(instances);</span>

      // remove instances with missing class
<span class="fc" id="L308">      instances = new Instances(instances);</span>
<span class="fc" id="L309">      instances.deleteWithMissingClass();</span>
      
<span class="fc" id="L311">        numClasses = instances.numClasses();</span>
<span class="fc" id="L312">	int numAttributes = instances.numAttributes();</span>
        
<span class="fc" id="L314">        header = new Instances(instances, 0);</span>
<span class="fc" id="L315">	double [][] ocrnceOfWordInClass = new double[numClasses][numAttributes];        </span>
<span class="fc" id="L316">        wordWeights = new double[numClasses][numAttributes];</span>
        //double [] docsPerClass = new double[numClasses];
<span class="fc" id="L318">	double[] wordsPerClass = new double[numClasses];</span>
<span class="fc" id="L319">        double totalWordOccurrences = 0;</span>
<span class="fc" id="L320">        double sumOfSmoothingParams = (numAttributes-1)*smoothingParameter;</span>
<span class="fc" id="L321">        int classIndex = instances.instance(0).classIndex();        </span>
	Instance instance;
	int docClass;
	double numOccurrences;        
        
<span class="fc" id="L326">        java.util.Enumeration enumInsts = instances.enumerateInstances();</span>
<span class="fc bfc" id="L327" title="All 2 branches covered.">	while (enumInsts.hasMoreElements()) {</span>
<span class="fc" id="L328">		instance = (Instance) enumInsts.nextElement();</span>
<span class="fc" id="L329">		docClass = (int)instance.value(classIndex);</span>
		//docsPerClass[docClass] += instance.weight();
		
<span class="fc bfc" id="L332" title="All 2 branches covered.">		for(int a = 0; a&lt;instance.numValues(); a++)</span>
<span class="fc bfc" id="L333" title="All 2 branches covered.">		    if(instance.index(a) != instance.classIndex()) {</span>
<span class="fc bfc" id="L334" title="All 2 branches covered.">			    if(!instance.isMissing(a)) {</span>
<span class="fc" id="L335">				    numOccurrences = instance.valueSparse(a) * instance.weight();</span>
<span class="pc bpc" id="L336" title="1 of 2 branches missed.">				    if(numOccurrences &lt; 0)</span>
<span class="nc" id="L337">					throw new Exception(&quot;Numeric attribute&quot;+</span>
                                                  &quot; values must all be greater&quot;+
                                                  &quot; or equal to zero.&quot;);
<span class="fc" id="L340">                                    totalWordOccurrences += numOccurrences;</span>
<span class="fc" id="L341">				    wordsPerClass[docClass] += numOccurrences;</span>
<span class="fc" id="L342">				    ocrnceOfWordInClass[docClass]</span>
<span class="fc" id="L343">                                          [instance.index(a)] += numOccurrences;</span>
                                    //For the time being wordweights[0][i] 
                                    //will hold the total occurrence of word
                                    // i over all classes
<span class="fc" id="L347">                                    wordWeights[0]</span>
<span class="fc" id="L348">                                          [instance.index(a)] += numOccurrences;</span>
                            }
                    }
        }

	//Calculating the complement class probability for all classes except 0        
<span class="fc bfc" id="L354" title="All 2 branches covered.">	for(int c=1; c&lt;numClasses; c++) {</span>
            //total occurrence of words in classes other than c
<span class="fc" id="L356">            double totalWordOcrnces = totalWordOccurrences - wordsPerClass[c];</span>

<span class="fc bfc" id="L358" title="All 2 branches covered.">            for(int w=0; w&lt;numAttributes; w++) {</span>
<span class="fc bfc" id="L359" title="All 2 branches covered.">                if(w != classIndex ) {</span>
                     //occurrence of w in classes other that c
<span class="fc" id="L361">                    double ocrncesOfWord = </span>
<span class="fc" id="L362">                                wordWeights[0][w] - ocrnceOfWordInClass[c][w];</span>

<span class="fc" id="L364">                    wordWeights[c][w] = </span>
<span class="fc" id="L365">                        Math.log((ocrncesOfWord+smoothingParameter) / </span>
<span class="fc" id="L366">                                (totalWordOcrnces+sumOfSmoothingParams));</span>
                }
            }
        }
        
	//Now calculating the complement class probability for class 0
<span class="fc bfc" id="L372" title="All 2 branches covered.">        for(int w=0; w&lt;numAttributes; w++) {</span>
<span class="fc bfc" id="L373" title="All 2 branches covered.">            if(w != classIndex) {</span>
                //occurrence of w in classes other that c
<span class="fc" id="L375">                double ocrncesOfWord = wordWeights[0][w] - ocrnceOfWordInClass[0][w];</span>
                //total occurrence of words in classes other than c
<span class="fc" id="L377">                double totalWordOcrnces = totalWordOccurrences - wordsPerClass[0];</span>
                
<span class="fc" id="L379">                wordWeights[0][w] =</span>
<span class="fc" id="L380">                Math.log((ocrncesOfWord+smoothingParameter) /</span>
<span class="fc" id="L381">                (totalWordOcrnces+sumOfSmoothingParams));</span>
            }            
        }
        
       	//Normalizing weights
<span class="pc bpc" id="L386" title="1 of 2 branches missed.">        if(m_normalizeWordWeights==true)</span>
<span class="nc bnc" id="L387" title="All 2 branches missed.">            for(int c=0; c&lt;numClasses; c++) {</span>
<span class="nc" id="L388">                double sum=0;</span>
<span class="nc bnc" id="L389" title="All 2 branches missed.">                for(int w=0; w&lt;numAttributes; w++) {</span>
<span class="nc bnc" id="L390" title="All 2 branches missed.">                    if(w!=classIndex)</span>
<span class="nc" id="L391">                        sum += Math.abs(wordWeights[c][w]);</span>
                }
<span class="nc bnc" id="L393" title="All 2 branches missed.">                for(int w=0; w&lt;numAttributes; w++) {</span>
<span class="nc bnc" id="L394" title="All 2 branches missed.">                    if(w!=classIndex) {</span>
<span class="nc" id="L395">                        wordWeights[c][w] = wordWeights[c][w]/sum;</span>
                    }
                }
            }

<span class="fc" id="L400">    }</span>

    
    /**
     * Classifies a given instance. &lt;p&gt;
     *
     * The classification rule is: &lt;br&gt;
     *     MinC(forAllWords(ti*Wci)) &lt;br&gt;
     *      where &lt;br&gt;
     *         ti is the frequency of word i in the given instance &lt;br&gt;
     *         Wci is the weight of word i in Class c. &lt;p&gt;
     *
     * For more information see section 4.4 of the paper mentioned above
     * in the classifiers description.
     *
     * @param instance the instance to classify
     * @return the index of the class the instance is most likely to belong.
     * @throws Exception if the classifier has not been built yet.
     */
    public double classifyInstance(Instance instance) throws Exception {

<span class="pc bpc" id="L421" title="1 of 2 branches missed.">        if(wordWeights==null)</span>
<span class="nc" id="L422">            throw new Exception(&quot;Error. The classifier has not been built &quot;+</span>
                                &quot;properly.&quot;);
        
<span class="fc" id="L425">        double [] valueForClass = new double[numClasses];</span>
<span class="fc" id="L426">	double sumOfClassValues=0;</span>
	
<span class="fc bfc" id="L428" title="All 2 branches covered.">	for(int c=0; c&lt;numClasses; c++) {</span>
<span class="fc" id="L429">	    double sumOfWordValues=0;</span>
<span class="fc bfc" id="L430" title="All 2 branches covered.">	    for(int w=0; w&lt;instance.numValues(); w++) {</span>
<span class="fc bfc" id="L431" title="All 2 branches covered.">                if(instance.index(w)!=instance.classIndex()) {</span>
<span class="fc" id="L432">                    double freqOfWordInDoc = instance.valueSparse(w);</span>
<span class="fc" id="L433">                    sumOfWordValues += freqOfWordInDoc * </span>
<span class="fc" id="L434">                                  wordWeights[c][instance.index(w)];</span>
                }
	    }
	    //valueForClass[c] = Math.log(probOfClass[c]) - sumOfWordValues;
<span class="fc" id="L438">	    valueForClass[c] = sumOfWordValues;</span>
<span class="fc" id="L439">	    sumOfClassValues += valueForClass[c];</span>
	}

<span class="fc" id="L442">        int minidx=0;</span>
<span class="fc bfc" id="L443" title="All 2 branches covered.">	for(int i=0; i&lt;numClasses; i++)</span>
<span class="fc bfc" id="L444" title="All 2 branches covered.">	    if(valueForClass[i]&lt;valueForClass[minidx])</span>
<span class="fc" id="L445">		minidx = i;</span>
	
<span class="fc" id="L447">	return minidx;</span>
    }


    /**
     * Prints out the internal model built by the classifier. In this case
     * it prints out the word weights calculated when building the classifier.
     */
    public String toString() {
<span class="pc bpc" id="L456" title="1 of 2 branches missed.">        if(wordWeights==null) {            </span>
<span class="fc" id="L457">            return &quot;The classifier hasn't been built yet.&quot;;</span>
        }
        
<span class="nc" id="L460">        int numAttributes = header.numAttributes();</span>
<span class="nc" id="L461">        StringBuffer result = new StringBuffer(&quot;The word weights for each class are: \n&quot;+</span>
                                               &quot;------------------------------------\n\t&quot;);
        
<span class="nc bnc" id="L464" title="All 2 branches missed.">        for(int c = 0; c&lt;numClasses; c++)</span>
<span class="nc" id="L465">            result.append(header.classAttribute().value(c)).append(&quot;\t&quot;);</span>
        
<span class="nc" id="L467">        result.append(&quot;\n&quot;);</span>
        
<span class="nc bnc" id="L469" title="All 2 branches missed.">        for(int w = 0; w&lt;numAttributes; w++) {</span>
<span class="nc" id="L470">            result.append(header.attribute(w).name()).append(&quot;\t&quot;);</span>
<span class="nc bnc" id="L471" title="All 2 branches missed.">            for(int c = 0; c&lt;numClasses; c++)</span>
<span class="nc" id="L472">                result.append(Double.toString(wordWeights[c][w])).append(&quot;\t&quot;);</span>
<span class="nc" id="L473">            result.append(&quot;\n&quot;);</span>
        }
        
<span class="nc" id="L476">        return result.toString();</span>
    }
    
    /**
     * Returns the revision string.
     * 
     * @return		the revision
     */
    public String getRevision() {
<span class="nc" id="L485">      return RevisionUtils.extract(&quot;$Revision: 5516 $&quot;);</span>
    }
    
    /**
     * Main method for testing this class.
     *
     * @param argv the options
     */
    public static void main(String [] argv) {
<span class="nc" id="L494">      runClassifier(new ComplementNaiveBayes(), argv);</span>
<span class="nc" id="L495">    }        </span>
}

</pre><div class="footer"><span class="right">Created with <a href="http://www.eclemma.org/jacoco">JaCoCo</a> 0.7.2.201409121644</span>AllTests (Nov 28, 2015 2:34:31 PM)</div></body></html>
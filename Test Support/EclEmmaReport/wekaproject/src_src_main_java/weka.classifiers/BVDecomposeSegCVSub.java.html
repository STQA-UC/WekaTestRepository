<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><meta http-equiv="Content-Type" content="text/html;charset=UTF-8"/><link rel="stylesheet" href="../../../.resources/report.css" type="text/css"/><link rel="shortcut icon" href="../../../.resources/report.gif" type="image/gif"/><title>BVDecomposeSegCVSub.java</title><link rel="stylesheet" href="../../../.resources/prettify.css" type="text/css"/><script type="text/javascript" src="../../../.resources/prettify.js"></script></head><body onload="window['PR_TAB_WIDTH']=4;prettyPrint()"><div class="breadcrumb" id="breadcrumb"><span class="info"><a href="../../../.sessions.html" class="el_session">Sessions</a></span><a href="../../../index.html" class="el_report">AllTests (Nov 28, 2015 2:34:31 PM)</a> &gt; <a href="../../index.html" class="el_group">wekaproject</a> &gt; <a href="../index.html" class="el_bundle">src/src/main/java</a> &gt; <a href="index.source.html" class="el_package">weka.classifiers</a> &gt; <span class="el_source">BVDecomposeSegCVSub.java</span></div><h1>BVDecomposeSegCVSub.java</h1><pre class="source lang-java linenums">/*
 *    This program is free software; you can redistribute it and/or modify
 *    it under the terms of the GNU General Public License as published by
 *    the Free Software Foundation; either version 2 of the License, or
 *    (at your option) any later version.
 *
 *    This program is distributed in the hope that it will be useful,
 *    but WITHOUT ANY WARRANTY; without even the implied warranty of
 *    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 *    GNU General Public License for more details.
 *
 *    You should have received a copy of the GNU General Public License
 *    along with this program; if not, write to the Free Software
 *    Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
 */

/*
 *    BVDecomposeSegCVSub.java
 *    Copyright (C) 2003 Paul Conilione
 *
 *    Based on the class: BVDecompose.java by Len Trigg (1999)
 */


/*
 *    DEDICATION
 *
 *    Paul Conilione would like to express his deep gratitude and appreciation
 *    to his Chinese Buddhist Taoist Master Sifu Chow Yuk Nen for the abilities
 *    and insight that he has been taught, which have allowed him to program in 
 *    a clear and efficient manner.
 *
 *    Master Sifu Chow Yuk Nen's Teachings are unique and precious. They are
 *    applicable to any field of human endeavour. Through his unique and powerful
 *    ability to skilfully apply Chinese Buddhist Teachings, people have achieved
 *    success in; Computing, chemical engineering, business, accounting, philosophy
 *    and more.
 *
 */

package weka.classifiers;

import weka.core.Attribute;
import weka.core.Instance;
import weka.core.Instances;
import weka.core.Option;
import weka.core.OptionHandler;
import weka.core.RevisionHandler;
import weka.core.RevisionUtils;
import weka.core.TechnicalInformation;
import weka.core.TechnicalInformationHandler;
import weka.core.Utils;
import weka.core.TechnicalInformation.Field;
import weka.core.TechnicalInformation.Type;

import java.io.BufferedReader;
import java.io.FileReader;
import java.io.Reader;
import java.util.Enumeration;
import java.util.Random;
import java.util.Vector;

/**
 &lt;!-- globalinfo-start --&gt;
 * This class performs Bias-Variance decomposion on any classifier using the sub-sampled cross-validation procedure as specified in (1).&lt;br/&gt;
 * The Kohavi and Wolpert definition of bias and variance is specified in (2).&lt;br/&gt;
 * The Webb definition of bias and variance is specified in (3).&lt;br/&gt;
 * &lt;br/&gt;
 * Geoffrey I. Webb, Paul Conilione (2002). Estimating bias and variance from data. School of Computer Science and Software Engineering, Victoria, Australia.&lt;br/&gt;
 * &lt;br/&gt;
 * Ron Kohavi, David H. Wolpert: Bias Plus Variance Decomposition for Zero-One Loss Functions. In: Machine Learning: Proceedings of the Thirteenth International Conference, 275-283, 1996.&lt;br/&gt;
 * &lt;br/&gt;
 * Geoffrey I. Webb (2000). MultiBoosting: A Technique for Combining Boosting and Wagging. Machine Learning. 40(2):159-196.
 * &lt;p/&gt;
 &lt;!-- globalinfo-end --&gt;
 * 
 &lt;!-- technical-bibtex-start --&gt;
 * BibTeX:
 * &lt;pre&gt;
 * &amp;#64;misc{Webb2002,
 *    address = {School of Computer Science and Software Engineering, Victoria, Australia},
 *    author = {Geoffrey I. Webb and Paul Conilione},
 *    institution = {Monash University},
 *    title = {Estimating bias and variance from data},
 *    year = {2002},
 *    PDF = {http://www.csse.monash.edu.au/\~webb/Files/WebbConilione04.pdf}
 * }
 * 
 * &amp;#64;inproceedings{Kohavi1996,
 *    author = {Ron Kohavi and David H. Wolpert},
 *    booktitle = {Machine Learning: Proceedings of the Thirteenth International Conference},
 *    editor = {Lorenza Saitta},
 *    pages = {275-283},
 *    publisher = {Morgan Kaufmann},
 *    title = {Bias Plus Variance Decomposition for Zero-One Loss Functions},
 *    year = {1996},
 *    PS = {http://robotics.stanford.edu/\~ronnyk/biasVar.ps}
 * }
 * 
 * &amp;#64;article{Webb2000,
 *    author = {Geoffrey I. Webb},
 *    journal = {Machine Learning},
 *    number = {2},
 *    pages = {159-196},
 *    title = {MultiBoosting: A Technique for Combining Boosting and Wagging},
 *    volume = {40},
 *    year = {2000}
 * }
 * &lt;/pre&gt;
 * &lt;p/&gt;
 &lt;!-- technical-bibtex-end --&gt;
 *
 &lt;!-- options-start --&gt;
 * Valid options are: &lt;p/&gt;
 * 
 * &lt;pre&gt; -c &amp;lt;class index&amp;gt;
 *  The index of the class attribute.
 *  (default last)&lt;/pre&gt;
 * 
 * &lt;pre&gt; -D
 *  Turn on debugging output.&lt;/pre&gt;
 * 
 * &lt;pre&gt; -l &amp;lt;num&amp;gt;
 *  The number of times each instance is classified.
 *  (default 10)&lt;/pre&gt;
 * 
 * &lt;pre&gt; -p &amp;lt;proportion of objects in common&amp;gt;
 *  The average proportion of instances common between any two training sets&lt;/pre&gt;
 * 
 * &lt;pre&gt; -s &amp;lt;seed&amp;gt;
 *  The random number seed used.&lt;/pre&gt;
 * 
 * &lt;pre&gt; -t &amp;lt;name of arff file&amp;gt;
 *  The name of the arff file used for the decomposition.&lt;/pre&gt;
 * 
 * &lt;pre&gt; -T &amp;lt;number of instances in training set&amp;gt;
 *  The number of instances in the training set.&lt;/pre&gt;
 * 
 * &lt;pre&gt; -W &amp;lt;classifier class name&amp;gt;
 *  Full class name of the learner used in the decomposition.
 *  eg: weka.classifiers.bayes.NaiveBayes&lt;/pre&gt;
 * 
 * &lt;pre&gt; 
 * Options specific to learner weka.classifiers.rules.ZeroR:
 * &lt;/pre&gt;
 * 
 * &lt;pre&gt; -D
 *  If set, classifier is run in debug mode and
 *  may output additional info to the console&lt;/pre&gt;
 * 
 &lt;!-- options-end --&gt;
 *
 * Options after -- are passed to the designated sub-learner. &lt;p&gt;
 *
 * @author Paul Conilione (paulc4321@yahoo.com.au)
 * @version $Revision: 1.7 $
 */
<span class="nc" id="L158">public class BVDecomposeSegCVSub</span>
    implements OptionHandler, TechnicalInformationHandler, RevisionHandler {
    
    /** Debugging mode, gives extra output if true. */
    protected boolean m_Debug;
    
    /** An instantiated base classifier used for getting and testing options. */
<span class="nc" id="L165">    protected Classifier m_Classifier = new weka.classifiers.rules.ZeroR();</span>
    
    /** The options to be passed to the base classifier. */
    protected String [] m_ClassifierOptions;
    
    /** The number of times an instance is classified*/
    protected int m_ClassifyIterations;
    
    /** The name of the data file used for the decomposition */
    protected String m_DataFileName;
    
    /** The index of the class attribute */
<span class="nc" id="L177">    protected int m_ClassIndex = -1;</span>
    
    /** The random number seed */
<span class="nc" id="L180">    protected int m_Seed = 1;</span>
    
    /** The calculated Kohavi &amp; Wolpert bias (squared) */
    protected double m_KWBias;
    
    /** The calculated Kohavi &amp; Wolpert variance */
    protected double m_KWVariance;
    
    /** The calculated Kohavi &amp; Wolpert sigma */
    protected double m_KWSigma;
    
    /** The calculated Webb bias */
    protected double m_WBias;
    
    /** The calculated Webb variance */
    protected double m_WVariance;
    
    /** The error rate */
    protected double m_Error;
    
    /** The training set size */
    protected int m_TrainSize;
    
    /** Proportion of instances common between any two training sets. */
    protected double m_P;
    
    /**
     * Returns a string describing this object
     * @return a description of the classifier suitable for
     * displaying in the explorer/experimenter gui
     */
    public String globalInfo() {
<span class="nc" id="L212">      return </span>
<span class="nc" id="L213">          &quot;This class performs Bias-Variance decomposion on any classifier using the &quot;</span>
        + &quot;sub-sampled cross-validation procedure as specified in (1).\n&quot;
        + &quot;The Kohavi and Wolpert definition of bias and variance is specified in (2).\n&quot;
        + &quot;The Webb definition of bias and variance is specified in (3).\n\n&quot;
<span class="nc" id="L217">        + getTechnicalInformation().toString();</span>
    }

    /**
     * Returns an instance of a TechnicalInformation object, containing 
     * detailed information about the technical background of this class,
     * e.g., paper reference or book this class is based on.
     * 
     * @return the technical information about this class
     */
    public TechnicalInformation getTechnicalInformation() {
      TechnicalInformation 	result;
      TechnicalInformation 	additional;
      
<span class="nc" id="L231">      result = new TechnicalInformation(Type.MISC);</span>
<span class="nc" id="L232">      result.setValue(Field.AUTHOR, &quot;Geoffrey I. Webb and Paul Conilione&quot;);</span>
<span class="nc" id="L233">      result.setValue(Field.YEAR, &quot;2002&quot;);</span>
<span class="nc" id="L234">      result.setValue(Field.TITLE, &quot;Estimating bias and variance from data&quot;);</span>
<span class="nc" id="L235">      result.setValue(Field.INSTITUTION, &quot;Monash University&quot;);</span>
<span class="nc" id="L236">      result.setValue(Field.ADDRESS, &quot;School of Computer Science and Software Engineering, Victoria, Australia&quot;);</span>
<span class="nc" id="L237">      result.setValue(Field.PDF, &quot;http://www.csse.monash.edu.au/~webb/Files/WebbConilione04.pdf&quot;);</span>

<span class="nc" id="L239">      additional = result.add(Type.INPROCEEDINGS);</span>
<span class="nc" id="L240">      additional.setValue(Field.AUTHOR, &quot;Ron Kohavi and David H. Wolpert&quot;);</span>
<span class="nc" id="L241">      additional.setValue(Field.YEAR, &quot;1996&quot;);</span>
<span class="nc" id="L242">      additional.setValue(Field.TITLE, &quot;Bias Plus Variance Decomposition for Zero-One Loss Functions&quot;);</span>
<span class="nc" id="L243">      additional.setValue(Field.BOOKTITLE, &quot;Machine Learning: Proceedings of the Thirteenth International Conference&quot;);</span>
<span class="nc" id="L244">      additional.setValue(Field.PUBLISHER, &quot;Morgan Kaufmann&quot;);</span>
<span class="nc" id="L245">      additional.setValue(Field.EDITOR, &quot;Lorenza Saitta&quot;);</span>
<span class="nc" id="L246">      additional.setValue(Field.PAGES, &quot;275-283&quot;);</span>
<span class="nc" id="L247">      additional.setValue(Field.PS, &quot;http://robotics.stanford.edu/~ronnyk/biasVar.ps&quot;);</span>

<span class="nc" id="L249">      additional = result.add(Type.ARTICLE);</span>
<span class="nc" id="L250">      additional.setValue(Field.AUTHOR, &quot;Geoffrey I. Webb&quot;);</span>
<span class="nc" id="L251">      additional.setValue(Field.YEAR, &quot;2000&quot;);</span>
<span class="nc" id="L252">      additional.setValue(Field.TITLE, &quot;MultiBoosting: A Technique for Combining Boosting and Wagging&quot;);</span>
<span class="nc" id="L253">      additional.setValue(Field.JOURNAL, &quot;Machine Learning&quot;);</span>
<span class="nc" id="L254">      additional.setValue(Field.VOLUME, &quot;40&quot;);</span>
<span class="nc" id="L255">      additional.setValue(Field.NUMBER, &quot;2&quot;);</span>
<span class="nc" id="L256">      additional.setValue(Field.PAGES, &quot;159-196&quot;);</span>

<span class="nc" id="L258">      return result;</span>
    }
    
    /**
     * Returns an enumeration describing the available options.
     *
     * @return an enumeration of all the available options.
     */
    public Enumeration listOptions() {
        
<span class="nc" id="L268">        Vector newVector = new Vector(8);</span>
        
<span class="nc" id="L270">        newVector.addElement(new Option(</span>
<span class="nc" id="L271">        &quot;\tThe index of the class attribute.\n&quot;+</span>
        &quot;\t(default last)&quot;,
<span class="nc" id="L273">        &quot;c&quot;, 1, &quot;-c &lt;class index&gt;&quot;));</span>
<span class="nc" id="L274">        newVector.addElement(new Option(</span>
<span class="nc" id="L275">        &quot;\tTurn on debugging output.&quot;,</span>
<span class="nc" id="L276">        &quot;D&quot;, 0, &quot;-D&quot;));</span>
<span class="nc" id="L277">        newVector.addElement(new Option(</span>
<span class="nc" id="L278">        &quot;\tThe number of times each instance is classified.\n&quot;</span>
        +&quot;\t(default 10)&quot;,
<span class="nc" id="L280">        &quot;l&quot;, 1, &quot;-l &lt;num&gt;&quot;));</span>
<span class="nc" id="L281">        newVector.addElement(new Option(</span>
<span class="nc" id="L282">        &quot;\tThe average proportion of instances common between any two training sets&quot;,</span>
<span class="nc" id="L283">        &quot;p&quot;, 1, &quot;-p &lt;proportion of objects in common&gt;&quot;));</span>
<span class="nc" id="L284">        newVector.addElement(new Option(</span>
<span class="nc" id="L285">        &quot;\tThe random number seed used.&quot;,</span>
<span class="nc" id="L286">        &quot;s&quot;, 1, &quot;-s &lt;seed&gt;&quot;));</span>
<span class="nc" id="L287">        newVector.addElement(new Option(</span>
<span class="nc" id="L288">        &quot;\tThe name of the arff file used for the decomposition.&quot;,</span>
<span class="nc" id="L289">        &quot;t&quot;, 1, &quot;-t &lt;name of arff file&gt;&quot;));</span>
<span class="nc" id="L290">        newVector.addElement(new Option(</span>
<span class="nc" id="L291">        &quot;\tThe number of instances in the training set.&quot;,</span>
<span class="nc" id="L292">        &quot;T&quot;, 1, &quot;-T &lt;number of instances in training set&gt;&quot;));</span>
<span class="nc" id="L293">        newVector.addElement(new Option(</span>
<span class="nc" id="L294">        &quot;\tFull class name of the learner used in the decomposition.\n&quot;</span>
        +&quot;\teg: weka.classifiers.bayes.NaiveBayes&quot;,
<span class="nc" id="L296">        &quot;W&quot;, 1, &quot;-W &lt;classifier class name&gt;&quot;));</span>
        
<span class="nc bnc" id="L298" title="All 2 branches missed.">        if ((m_Classifier != null) &amp;&amp;</span>
<span class="nc bnc" id="L299" title="All 2 branches missed.">        (m_Classifier instanceof OptionHandler)) {</span>
<span class="nc" id="L300">            newVector.addElement(new Option(</span>
<span class="nc" id="L301">            &quot;&quot;,</span>
<span class="nc" id="L302">            &quot;&quot;, 0, &quot;\nOptions specific to learner &quot;</span>
<span class="nc" id="L303">            + m_Classifier.getClass().getName()</span>
<span class="nc" id="L304">            + &quot;:&quot;));</span>
<span class="nc" id="L305">            Enumeration enu = ((OptionHandler)m_Classifier).listOptions();</span>
<span class="nc bnc" id="L306" title="All 2 branches missed.">            while (enu.hasMoreElements()) {</span>
<span class="nc" id="L307">                newVector.addElement(enu.nextElement());</span>
            }
        }
<span class="nc" id="L310">        return newVector.elements();</span>
    }
    
    
    /** 
     * Sets the OptionHandler's options using the given list. All options
     * will be set (or reset) during this call (i.e. incremental setting
     * of options is not possible). &lt;p/&gt;
     *
     &lt;!-- options-start --&gt;
     * Valid options are: &lt;p/&gt;
     * 
     * &lt;pre&gt; -c &amp;lt;class index&amp;gt;
     *  The index of the class attribute.
     *  (default last)&lt;/pre&gt;
     * 
     * &lt;pre&gt; -D
     *  Turn on debugging output.&lt;/pre&gt;
     * 
     * &lt;pre&gt; -l &amp;lt;num&amp;gt;
     *  The number of times each instance is classified.
     *  (default 10)&lt;/pre&gt;
     * 
     * &lt;pre&gt; -p &amp;lt;proportion of objects in common&amp;gt;
     *  The average proportion of instances common between any two training sets&lt;/pre&gt;
     * 
     * &lt;pre&gt; -s &amp;lt;seed&amp;gt;
     *  The random number seed used.&lt;/pre&gt;
     * 
     * &lt;pre&gt; -t &amp;lt;name of arff file&amp;gt;
     *  The name of the arff file used for the decomposition.&lt;/pre&gt;
     * 
     * &lt;pre&gt; -T &amp;lt;number of instances in training set&amp;gt;
     *  The number of instances in the training set.&lt;/pre&gt;
     * 
     * &lt;pre&gt; -W &amp;lt;classifier class name&amp;gt;
     *  Full class name of the learner used in the decomposition.
     *  eg: weka.classifiers.bayes.NaiveBayes&lt;/pre&gt;
     * 
     * &lt;pre&gt; 
     * Options specific to learner weka.classifiers.rules.ZeroR:
     * &lt;/pre&gt;
     * 
     * &lt;pre&gt; -D
     *  If set, classifier is run in debug mode and
     *  may output additional info to the console&lt;/pre&gt;
     * 
     &lt;!-- options-end --&gt;
     *
     * @param options the list of options as an array of strings
     * @throws Exception if an option is not supported
     */
    public void setOptions(String[] options) throws Exception {
<span class="nc" id="L363">        setDebug(Utils.getFlag('D', options));</span>
        
<span class="nc" id="L365">        String classIndex = Utils.getOption('c', options);</span>
<span class="nc bnc" id="L366" title="All 2 branches missed.">        if (classIndex.length() != 0) {</span>
<span class="nc bnc" id="L367" title="All 2 branches missed.">            if (classIndex.toLowerCase().equals(&quot;last&quot;)) {</span>
<span class="nc" id="L368">                setClassIndex(0);</span>
<span class="nc bnc" id="L369" title="All 2 branches missed.">            } else if (classIndex.toLowerCase().equals(&quot;first&quot;)) {</span>
<span class="nc" id="L370">                setClassIndex(1);</span>
            } else {
<span class="nc" id="L372">                setClassIndex(Integer.parseInt(classIndex));</span>
            }
        } else {
<span class="nc" id="L375">            setClassIndex(0);</span>
        }
        
<span class="nc" id="L378">        String classifyIterations = Utils.getOption('l', options);</span>
<span class="nc bnc" id="L379" title="All 2 branches missed.">        if (classifyIterations.length() != 0) {</span>
<span class="nc" id="L380">            setClassifyIterations(Integer.parseInt(classifyIterations));</span>
        } else {
<span class="nc" id="L382">            setClassifyIterations(10);</span>
        }
        
<span class="nc" id="L385">        String prob = Utils.getOption('p', options);</span>
<span class="nc bnc" id="L386" title="All 2 branches missed.">        if (prob.length() != 0) {</span>
<span class="nc" id="L387">            setP( Double.parseDouble(prob));</span>
        } else {
<span class="nc" id="L389">            setP(-1);</span>
        }
        //throw new Exception(&quot;A proportion must be specified&quot; + &quot; with a -p option.&quot;);
        
<span class="nc" id="L393">        String seedString = Utils.getOption('s', options);</span>
<span class="nc bnc" id="L394" title="All 2 branches missed.">        if (seedString.length() != 0) {</span>
<span class="nc" id="L395">            setSeed(Integer.parseInt(seedString));</span>
        } else {
<span class="nc" id="L397">            setSeed(1);</span>
        }
        
<span class="nc" id="L400">        String dataFile = Utils.getOption('t', options);</span>
<span class="nc bnc" id="L401" title="All 2 branches missed.">        if (dataFile.length() != 0) {</span>
<span class="nc" id="L402">            setDataFileName(dataFile);</span>
        } else {
<span class="nc" id="L404">            throw new Exception(&quot;An arff file must be specified&quot;</span>
            + &quot; with the -t option.&quot;);
        }
        
<span class="nc" id="L408">        String trainSize = Utils.getOption('T', options);</span>
<span class="nc bnc" id="L409" title="All 2 branches missed.">        if (trainSize.length() != 0) {</span>
<span class="nc" id="L410">            setTrainSize(Integer.parseInt(trainSize));</span>
        } else {
<span class="nc" id="L412">            setTrainSize(-1);</span>
        }
        //throw new Exception(&quot;A training set size must be specified&quot; + &quot; with a -T option.&quot;);
        
<span class="nc" id="L416">        String classifierName = Utils.getOption('W', options);</span>
<span class="nc bnc" id="L417" title="All 2 branches missed.">        if (classifierName.length() != 0) {</span>
<span class="nc" id="L418">            setClassifier(Classifier.forName(classifierName, Utils.partitionOptions(options)));</span>
        } else {
<span class="nc" id="L420">            throw new Exception(&quot;A learner must be specified with the -W option.&quot;);</span>
        }
<span class="nc" id="L422">    }</span>
    
    /**
     * Gets the current settings of the CheckClassifier.
     *
     * @return an array of strings suitable for passing to setOptions
     */
    public String [] getOptions() {
        
<span class="nc" id="L431">        String [] classifierOptions = new String [0];</span>
<span class="nc bnc" id="L432" title="All 2 branches missed.">        if ((m_Classifier != null) &amp;&amp;</span>
<span class="nc bnc" id="L433" title="All 2 branches missed.">        (m_Classifier instanceof OptionHandler)) {</span>
<span class="nc" id="L434">            classifierOptions = ((OptionHandler)m_Classifier).getOptions();</span>
        }
<span class="nc" id="L436">        String [] options = new String [classifierOptions.length + 14];</span>
<span class="nc" id="L437">        int current = 0;</span>
<span class="nc bnc" id="L438" title="All 2 branches missed.">        if (getDebug()) {</span>
<span class="nc" id="L439">            options[current++] = &quot;-D&quot;;</span>
        }
<span class="nc" id="L441">        options[current++] = &quot;-c&quot;; options[current++] = &quot;&quot; + getClassIndex();</span>
<span class="nc" id="L442">        options[current++] = &quot;-l&quot;; options[current++] = &quot;&quot; + getClassifyIterations();</span>
<span class="nc" id="L443">        options[current++] = &quot;-p&quot;; options[current++] = &quot;&quot; + getP();</span>
<span class="nc" id="L444">        options[current++] = &quot;-s&quot;; options[current++] = &quot;&quot; + getSeed();</span>
<span class="nc bnc" id="L445" title="All 2 branches missed.">        if (getDataFileName() != null) {</span>
<span class="nc" id="L446">            options[current++] = &quot;-t&quot;; options[current++] = &quot;&quot; + getDataFileName();</span>
        }
<span class="nc" id="L448">        options[current++] = &quot;-T&quot;; options[current++] = &quot;&quot; + getTrainSize();</span>
<span class="nc bnc" id="L449" title="All 2 branches missed.">        if (getClassifier() != null) {</span>
<span class="nc" id="L450">            options[current++] = &quot;-W&quot;;</span>
<span class="nc" id="L451">            options[current++] = getClassifier().getClass().getName();</span>
        }
        
<span class="nc" id="L454">        options[current++] = &quot;--&quot;;</span>
<span class="nc" id="L455">        System.arraycopy(classifierOptions, 0, options, current,</span>
<span class="nc" id="L456">        classifierOptions.length);</span>
<span class="nc" id="L457">        current += classifierOptions.length;</span>
<span class="nc bnc" id="L458" title="All 2 branches missed.">        while (current &lt; options.length) {</span>
<span class="nc" id="L459">            options[current++] = &quot;&quot;;</span>
        }
<span class="nc" id="L461">        return options;</span>
    }
    
    /**
     * Set the classifiers being analysed
     *
     * @param newClassifier the Classifier to use.
     */
    public void setClassifier(Classifier newClassifier) {
        
<span class="nc" id="L471">        m_Classifier = newClassifier;</span>
<span class="nc" id="L472">    }</span>
    
    /**
     * Gets the name of the classifier being analysed
     *
     * @return the classifier being analysed.
     */
    public Classifier getClassifier() {
        
<span class="nc" id="L481">        return m_Classifier;</span>
    }
    
    /**
     * Sets debugging mode
     *
     * @param debug true if debug output should be printed
     */
    public void setDebug(boolean debug) {
        
<span class="nc" id="L491">        m_Debug = debug;</span>
<span class="nc" id="L492">    }</span>
    
    /**
     * Gets whether debugging is turned on
     *
     * @return true if debugging output is on
     */
    public boolean getDebug() {
        
<span class="nc" id="L501">        return m_Debug;</span>
    }
    
    
    /**
     * Sets the random number seed
     * 
     * @param seed the random number seed
     */
    public void setSeed(int seed) {
        
<span class="nc" id="L512">        m_Seed = seed;</span>
<span class="nc" id="L513">    }</span>
    
    /**
     * Gets the random number seed
     *
     * @return the random number seed
     */
    public int getSeed() {
        
<span class="nc" id="L522">        return m_Seed;</span>
    }
    
    /**
     * Sets the number of times an instance is classified
     *
     * @param classifyIterations number of times an instance is classified
     */
    public void setClassifyIterations(int classifyIterations) {
        
<span class="nc" id="L532">        m_ClassifyIterations = classifyIterations;</span>
<span class="nc" id="L533">    }</span>
    
    /**
     * Gets the number of times an instance is classified
     *
     * @return the maximum number of times an instance is classified
     */
    public int getClassifyIterations() {
        
<span class="nc" id="L542">        return m_ClassifyIterations;</span>
    }
    
    /**
     * Sets the name of the dataset file.
     *
     * @param dataFileName name of dataset file.
     */
    public void setDataFileName(String dataFileName) {
        
<span class="nc" id="L552">        m_DataFileName = dataFileName;</span>
<span class="nc" id="L553">    }</span>
    
    /**
     * Get the name of the data file used for the decomposition
     *
     * @return the name of the data file
     */
    public String getDataFileName() {
        
<span class="nc" id="L562">        return m_DataFileName;</span>
    }
    
    /**
     * Get the index (starting from 1) of the attribute used as the class.
     *
     * @return the index of the class attribute
     */
    public int getClassIndex() {
        
<span class="nc" id="L572">        return m_ClassIndex + 1;</span>
    }
    
    /**
     * Sets index of attribute to discretize on
     *
     * @param classIndex the index (starting from 1) of the class attribute
     */
    public void setClassIndex(int classIndex) {
        
<span class="nc" id="L582">        m_ClassIndex = classIndex - 1;</span>
<span class="nc" id="L583">    }</span>
    
    /**
     * Get the calculated bias squared according to the Kohavi and Wolpert definition
     *
     * @return the bias squared
     */
    public double getKWBias() {
        
<span class="nc" id="L592">        return m_KWBias;</span>
    }
    
    /**
     * Get the calculated bias according to the Webb definition
     *
     * @return the bias
     *
     */
    public double getWBias() {
        
<span class="nc" id="L603">        return m_WBias;</span>
    }
    
    
    /**
     * Get the calculated variance according to the Kohavi and Wolpert definition
     *
     * @return the variance
     */
    public double getKWVariance() {
        
<span class="nc" id="L614">        return m_KWVariance;</span>
    }
    
    /**
     * Get the calculated variance according to the Webb definition
     *
     * @return the variance according to Webb
     *
     */
    public double getWVariance() {
        
<span class="nc" id="L625">        return m_WVariance;</span>
    }
    
    /**
     * Get the calculated sigma according to the Kohavi and Wolpert definition
     *
     * @return the sigma
     *
     */
    public double getKWSigma() {
        
<span class="nc" id="L636">        return m_KWSigma;</span>
    }
    
    /**
     * Set the training size.
     *
     * @param size the size of the training set
     *
     */
    public void setTrainSize(int size) {
        
<span class="nc" id="L647">        m_TrainSize = size;</span>
<span class="nc" id="L648">    }</span>
    
    /**
     * Get the training size
     *
     * @return the size of the training set
     *
     */
    public int getTrainSize() {
        
<span class="nc" id="L658">        return m_TrainSize;</span>
    }
    
    /**
     * Set the proportion of instances that are common between two training sets
     * used to train a classifier.
     *
     * @param proportion the proportion of instances that are common between training
     * sets.
     *
     */
    public void setP(double proportion) {
        
<span class="nc" id="L671">        m_P = proportion;</span>
<span class="nc" id="L672">    }</span>
    
    /**
     * Get the proportion of instances that are common between two training sets.
     *
     * @return the proportion
     *
     */
    public double getP() {
        
<span class="nc" id="L682">        return m_P;</span>
    }
    
    /**
     * Get the calculated error rate
     *
     * @return the error rate
     */
    public double getError() {
        
<span class="nc" id="L692">        return m_Error;</span>
    }
    
    /**
     * Carry out the bias-variance decomposition using the sub-sampled cross-validation method.
     *
     * @throws Exception if the decomposition couldn't be carried out
     */
    public void decompose() throws Exception {
        
        Reader dataReader;
        Instances data;
        
        int tps; // training pool size, size of segment E.
        int k; // number of folds in segment E.
        int q; // number of segments of size tps.
        
<span class="nc" id="L709">        dataReader = new BufferedReader(new FileReader(m_DataFileName)); //open file</span>
<span class="nc" id="L710">        data = new Instances(dataReader); // encapsulate in wrapper class called weka.Instances()</span>
        
<span class="nc bnc" id="L712" title="All 2 branches missed.">        if (m_ClassIndex &lt; 0) {</span>
<span class="nc" id="L713">            data.setClassIndex(data.numAttributes() - 1);</span>
        } else {
<span class="nc" id="L715">            data.setClassIndex(m_ClassIndex);</span>
        }
        
<span class="nc bnc" id="L718" title="All 2 branches missed.">        if (data.classAttribute().type() != Attribute.NOMINAL) {</span>
<span class="nc" id="L719">            throw new Exception(&quot;Class attribute must be nominal&quot;);</span>
        }
<span class="nc" id="L721">        int numClasses = data.numClasses();</span>
        
<span class="nc" id="L723">        data.deleteWithMissingClass();</span>
<span class="nc bnc" id="L724" title="All 2 branches missed.">        if ( data.checkForStringAttributes() ) {</span>
<span class="nc" id="L725">            throw new Exception(&quot;Can't handle string attributes!&quot;);</span>
        }
        
        // Dataset size must be greater than 2
<span class="nc bnc" id="L729" title="All 2 branches missed.">        if ( data.numInstances() &lt;= 2 ){</span>
<span class="nc" id="L730">            throw new Exception(&quot;Dataset size must be greater than 2.&quot;);</span>
        }
        
<span class="nc bnc" id="L733" title="All 2 branches missed.">        if ( m_TrainSize == -1 ){ // default value</span>
<span class="nc" id="L734">            m_TrainSize = (int) Math.floor( (double) data.numInstances() / 2.0 );</span>
<span class="nc bnc" id="L735" title="All 4 branches missed.">        }else  if ( m_TrainSize &lt; 0 || m_TrainSize &gt;= data.numInstances() - 1 ) {  // Check if 0 &lt; training Size &lt; D - 1</span>
<span class="nc" id="L736">            throw new Exception(&quot;Training set size of &quot;+m_TrainSize+&quot; is invalid.&quot;);</span>
        }
        
<span class="nc bnc" id="L739" title="All 2 branches missed.">        if ( m_P == -1 ){ // default value</span>
<span class="nc" id="L740">            m_P = (double) m_TrainSize / ( (double)data.numInstances() - 1 );</span>
<span class="nc bnc" id="L741" title="All 4 branches missed.">        }else if (  m_P &lt; ( m_TrainSize / ( (double)data.numInstances() - 1 ) ) || m_P &gt;= 1.0  ) { //Check if p is in range: m/(|D|-1) &lt;= p &lt; 1.0</span>
<span class="nc" id="L742">            throw new Exception(&quot;Proportion is not in range: &quot;+ (m_TrainSize / ((double) data.numInstances() - 1 )) +&quot; &lt;= p &lt; 1.0 &quot;);</span>
        }
        
        //roundup tps from double to integer
<span class="nc" id="L746">        tps = (int) Math.ceil( ((double)m_TrainSize / (double)m_P) + 1 );</span>
<span class="nc" id="L747">        k = (int) Math.ceil( tps / (tps - (double) m_TrainSize));</span>
        
        // number of folds cannot be more than the number of instances in the training pool
<span class="nc bnc" id="L750" title="All 2 branches missed.">        if ( k &gt; tps ) {</span>
<span class="nc" id="L751">            throw new Exception(&quot;The required number of folds is too many.&quot;</span>
            + &quot;Change p or the size of the training set.&quot;);
        }
        
        // calculate the number of segments, round down.
<span class="nc" id="L756">        q = (int) Math.floor( (double) data.numInstances() / (double)tps );</span>
        
        //create confusion matrix, columns = number of instances in data set, as all will be used,  by rows = number of classes.
<span class="nc" id="L759">        double [][] instanceProbs = new double [data.numInstances()][numClasses];</span>
<span class="nc" id="L760">        int [][] foldIndex = new int [ k ][ 2 ];</span>
<span class="nc" id="L761">        Vector segmentList = new Vector(q + 1);</span>
        
        //Set random seed
<span class="nc" id="L764">        Random random = new Random(m_Seed);</span>
        
<span class="nc" id="L766">        data.randomize(random);</span>
        
        //create index arrays for different segments
        
<span class="nc" id="L770">        int currentDataIndex = 0;</span>

<span class="nc bnc" id="L772" title="All 2 branches missed.">        for( int count = 1; count &lt;= (q + 1); count++ ){</span>
<span class="nc bnc" id="L773" title="All 2 branches missed.">            if( count &gt; q){</span>
<span class="nc" id="L774">                int [] segmentIndex = new int [ (data.numInstances() - (q * tps)) ];</span>
<span class="nc bnc" id="L775" title="All 2 branches missed.">                for(int index = 0; index &lt; segmentIndex.length; index++, currentDataIndex++){</span>
                    
<span class="nc" id="L777">                    segmentIndex[index] = currentDataIndex;</span>
                }
<span class="nc" id="L779">                segmentList.add(segmentIndex);</span>
            } else {
<span class="nc" id="L781">                int [] segmentIndex = new int [ tps ];</span>
                
<span class="nc bnc" id="L783" title="All 2 branches missed.">                for(int index = 0; index &lt; segmentIndex.length; index++, currentDataIndex++){</span>
<span class="nc" id="L784">                    segmentIndex[index] = currentDataIndex;</span>
                }
<span class="nc" id="L786">                segmentList.add(segmentIndex);</span>
            }
        }
        
<span class="nc" id="L790">        int remainder = tps % k; // remainder is used to determine when to shrink the fold size by 1.</span>
        
        //foldSize = ROUNDUP( tps / k ) (round up, eg 3 -&gt; 3,  3.3-&gt;4)
<span class="nc" id="L793">        int foldSize = (int) Math.ceil( (double)tps /(double) k); //roundup fold size double to integer</span>
<span class="nc" id="L794">        int index = 0;</span>
        int currentIndex;
        
<span class="nc bnc" id="L797" title="All 2 branches missed.">        for( int count = 0; count &lt; k; count ++){</span>
<span class="nc bnc" id="L798" title="All 4 branches missed.">            if( remainder != 0 &amp;&amp; count == remainder ){</span>
<span class="nc" id="L799">                foldSize -= 1;</span>
            }
<span class="nc" id="L801">            foldIndex[count][0] = index;</span>
<span class="nc" id="L802">            foldIndex[count][1] = foldSize;</span>
<span class="nc" id="L803">            index += foldSize;</span>
        }
        
<span class="nc bnc" id="L806" title="All 2 branches missed.">        for( int l = 0; l &lt; m_ClassifyIterations; l++) {</span>
            
<span class="nc bnc" id="L808" title="All 2 branches missed.">            for(int i = 1; i &lt;= q; i++){</span>
                
<span class="nc" id="L810">                int [] currentSegment = (int[]) segmentList.get(i - 1);</span>
                
<span class="nc" id="L812">                randomize(currentSegment, random);</span>
                
                //CROSS FOLD VALIDATION for current Segment
<span class="nc bnc" id="L815" title="All 2 branches missed.">                for( int j = 1; j &lt;= k; j++){</span>
                    
<span class="nc" id="L817">                    Instances TP = null;</span>
<span class="nc bnc" id="L818" title="All 2 branches missed.">                    for(int foldNum = 1; foldNum &lt;= k; foldNum++){</span>
<span class="nc bnc" id="L819" title="All 2 branches missed.">                        if( foldNum != j){</span>
                            
<span class="nc" id="L821">                            int startFoldIndex = foldIndex[ foldNum - 1 ][ 0 ]; //start index</span>
<span class="nc" id="L822">                            foldSize = foldIndex[ foldNum - 1 ][ 1 ];</span>
<span class="nc" id="L823">                            int endFoldIndex = startFoldIndex + foldSize - 1;</span>
                            
<span class="nc bnc" id="L825" title="All 2 branches missed.">                            for(int currentFoldIndex = startFoldIndex; currentFoldIndex &lt;= endFoldIndex; currentFoldIndex++){</span>
                                
<span class="nc bnc" id="L827" title="All 2 branches missed.">                                if( TP == null ){</span>
<span class="nc" id="L828">                                    TP = new Instances(data, currentSegment[ currentFoldIndex ], 1);</span>
                                }else{
<span class="nc" id="L830">                                    TP.add( data.instance( currentSegment[ currentFoldIndex ] ) );</span>
                                }
                            }
                        }
                    }
                    
<span class="nc" id="L836">                    TP.randomize(random);</span>
                    
<span class="nc bnc" id="L838" title="All 2 branches missed.">                    if( getTrainSize() &gt; TP.numInstances() ){</span>
<span class="nc" id="L839">                        throw new Exception(&quot;The training set size of &quot; + getTrainSize() + &quot;, is greater than the training pool &quot;</span>
<span class="nc" id="L840">                        + TP.numInstances() );</span>
                    }
                    
<span class="nc" id="L843">                    Instances train = new Instances(TP, 0, m_TrainSize);</span>
                    
<span class="nc" id="L845">		    Classifier current = Classifier.makeCopy(m_Classifier);</span>
<span class="nc" id="L846">                    current.buildClassifier(train); // create a clssifier using the instances in train.</span>
                    
<span class="nc" id="L848">                    int currentTestIndex = foldIndex[ j - 1 ][ 0 ]; //start index</span>
<span class="nc" id="L849">                    int testFoldSize = foldIndex[ j - 1 ][ 1 ]; //size</span>
<span class="nc" id="L850">                    int endTestIndex = currentTestIndex + testFoldSize - 1;</span>
                    
<span class="nc bnc" id="L852" title="All 2 branches missed.">                    while( currentTestIndex &lt;= endTestIndex ){</span>
                        
<span class="nc" id="L854">                        Instance testInst = data.instance( currentSegment[currentTestIndex] );</span>
<span class="nc" id="L855">                        int pred = (int)current.classifyInstance( testInst );</span>
                        
                        
<span class="nc bnc" id="L858" title="All 2 branches missed.">                        if(pred != testInst.classValue()) {</span>
<span class="nc" id="L859">                            m_Error++; // add 1 to mis-classifications.</span>
                        }
<span class="nc" id="L861">                        instanceProbs[ currentSegment[ currentTestIndex ] ][ pred ]++;</span>
<span class="nc" id="L862">                        currentTestIndex++;</span>
                    }
                    
<span class="nc bnc" id="L865" title="All 4 branches missed.">                    if( i == 1 &amp;&amp; j == 1){</span>
<span class="nc" id="L866">                        int[] segmentElast = (int[])segmentList.lastElement();</span>
<span class="nc bnc" id="L867" title="All 2 branches missed.">                        for( currentIndex = 0; currentIndex &lt; segmentElast.length; currentIndex++){</span>
<span class="nc" id="L868">                            Instance testInst = data.instance( segmentElast[currentIndex] );</span>
<span class="nc" id="L869">                            int pred = (int)current.classifyInstance( testInst );</span>
<span class="nc bnc" id="L870" title="All 2 branches missed.">                            if(pred != testInst.classValue()) {</span>
<span class="nc" id="L871">                                m_Error++; // add 1 to mis-classifications.</span>
                            }
                            
<span class="nc" id="L874">                            instanceProbs[ segmentElast[ currentIndex ] ][ pred ]++;</span>
                        }
                    }
                }
            }
        }
        
<span class="nc" id="L881">        m_Error /= (double)( m_ClassifyIterations * data.numInstances() );</span>
        
<span class="nc" id="L883">        m_KWBias = 0.0;</span>
<span class="nc" id="L884">        m_KWVariance = 0.0;</span>
<span class="nc" id="L885">        m_KWSigma = 0.0;</span>
        
<span class="nc" id="L887">        m_WBias = 0.0;</span>
<span class="nc" id="L888">        m_WVariance = 0.0;</span>
        
<span class="nc bnc" id="L890" title="All 2 branches missed.">        for (int i = 0; i &lt; data.numInstances(); i++) {</span>
            
<span class="nc" id="L892">            Instance current = data.instance( i );</span>
            
<span class="nc" id="L894">            double [] predProbs = instanceProbs[ i ];</span>
            double pActual, pPred;
<span class="nc" id="L896">            double bsum = 0, vsum = 0, ssum = 0;</span>
<span class="nc" id="L897">            double wBSum = 0, wVSum = 0;</span>
            
<span class="nc" id="L899">            Vector centralTendencies = findCentralTendencies( predProbs );</span>
            
<span class="nc bnc" id="L901" title="All 2 branches missed.">            if( centralTendencies == null ){</span>
<span class="nc" id="L902">                throw new Exception(&quot;Central tendency was null.&quot;);</span>
            }
            
<span class="nc bnc" id="L905" title="All 2 branches missed.">            for (int j = 0; j &lt; numClasses; j++) {</span>
<span class="nc bnc" id="L906" title="All 2 branches missed.">                pActual = (current.classValue() == j) ? 1 : 0;</span>
<span class="nc" id="L907">                pPred = predProbs[j] / m_ClassifyIterations;</span>
<span class="nc" id="L908">                bsum += (pActual - pPred) * (pActual - pPred) - pPred * (1 - pPred) / (m_ClassifyIterations - 1);</span>
<span class="nc" id="L909">                vsum += pPred * pPred;</span>
<span class="nc" id="L910">                ssum += pActual * pActual;</span>
            }
            
<span class="nc" id="L913">            m_KWBias += bsum;</span>
<span class="nc" id="L914">            m_KWVariance += (1 - vsum);</span>
<span class="nc" id="L915">            m_KWSigma += (1 - ssum);</span>
            
<span class="nc bnc" id="L917" title="All 2 branches missed.">            for( int count = 0; count &lt; centralTendencies.size(); count++ ) {</span>
                
<span class="nc" id="L919">                int wB = 0, wV = 0;</span>
<span class="nc" id="L920">                int centralTendency = ((Integer)centralTendencies.get(count)).intValue();</span>
                
                // For a single instance xi, find the bias and variance.
<span class="nc bnc" id="L923" title="All 2 branches missed.">                for (int j = 0; j &lt; numClasses; j++) {</span>
                    
                    //Webb definition
<span class="nc bnc" id="L926" title="All 4 branches missed.">                    if( j != (int)current.classValue() &amp;&amp; j == centralTendency ) {</span>
<span class="nc" id="L927">                        wB += predProbs[j];</span>
                    }
<span class="nc bnc" id="L929" title="All 4 branches missed.">                    if( j != (int)current.classValue() &amp;&amp; j != centralTendency ) {</span>
<span class="nc" id="L930">                        wV += predProbs[j];</span>
                    }
                    
                }
<span class="nc" id="L934">                wBSum += (double) wB;</span>
<span class="nc" id="L935">                wVSum += (double) wV;</span>
            }
            
            // calculate bais by dividing bSum by the number of central tendencies and
            // total number of instances. (effectively finding the average and dividing
            // by the number of instances to get the nominalised probability).
            
<span class="nc" id="L942">            m_WBias += ( wBSum / ((double) ( centralTendencies.size() * m_ClassifyIterations )));</span>
            // calculate variance by dividing vSum by the total number of interations
<span class="nc" id="L944">            m_WVariance += ( wVSum / ((double) ( centralTendencies.size() * m_ClassifyIterations )));</span>
            
        }
        
<span class="nc" id="L948">        m_KWBias /= (2.0 * (double) data.numInstances());</span>
<span class="nc" id="L949">        m_KWVariance /= (2.0 * (double) data.numInstances());</span>
<span class="nc" id="L950">        m_KWSigma /= (2.0 * (double) data.numInstances());</span>
        
        // bias = bias / number of data instances
<span class="nc" id="L953">        m_WBias /= (double) data.numInstances();</span>
        // variance = variance / number of data instances.
<span class="nc" id="L955">        m_WVariance /= (double) data.numInstances();</span>
        
<span class="nc bnc" id="L957" title="All 2 branches missed.">        if (m_Debug) {</span>
<span class="nc" id="L958">            System.err.println(&quot;Decomposition finished&quot;);</span>
        }
        
<span class="nc" id="L961">    }</span>
    
    /** Finds the central tendency, given the classifications for an instance.
     *
     * Where the central tendency is defined as the class that was most commonly
     * selected for a given instance.&lt;p&gt;
     *
     * For example, instance 'x' may be classified out of 3 classes y = {1, 2, 3},
     * so if x is classified 10 times, and is classified as follows, '1' = 2 times, '2' = 5 times
     * and '3' = 3 times. Then the central tendency is '2'. &lt;p&gt;
     *
     * However, it is important to note that this method returns a list of all classes
     * that have the highest number of classifications.
     *
     * In cases where there are several classes with the largest number of classifications, then
     * all of these classes are returned. For example if 'x' is classified '1' = 4 times,
     * '2' = 4 times and '3' = 2 times. Then '1' and '2' are returned.&lt;p&gt;
     *
     * @param predProbs the array of classifications for a single instance.
     *
     * @return a Vector containing Integer objects which store the class(s) which
     * are the central tendency.
     */
    public Vector findCentralTendencies(double[] predProbs) {
        
<span class="nc" id="L986">        int centralTValue = 0;</span>
<span class="nc" id="L987">        int currentValue = 0;</span>
        //array to store the list of classes the have the greatest number of classifictions.
        Vector centralTClasses;
        
<span class="nc" id="L991">        centralTClasses = new Vector(); //create an array with size of the number of classes.</span>
        
        // Go through array, finding the central tendency.
<span class="nc bnc" id="L994" title="All 2 branches missed.">        for( int i = 0; i &lt; predProbs.length; i++) {</span>
<span class="nc" id="L995">            currentValue = (int) predProbs[i];</span>
            // if current value is greater than the central tendency value then
            // clear vector and add new class to vector array.
<span class="nc bnc" id="L998" title="All 2 branches missed.">            if( currentValue &gt; centralTValue) {</span>
<span class="nc" id="L999">                centralTClasses.clear();</span>
<span class="nc" id="L1000">                centralTClasses.addElement( new Integer(i) );</span>
<span class="nc" id="L1001">                centralTValue = currentValue;</span>
<span class="nc bnc" id="L1002" title="All 4 branches missed.">            } else if( currentValue != 0 &amp;&amp; currentValue == centralTValue) {</span>
<span class="nc" id="L1003">                centralTClasses.addElement( new Integer(i) );</span>
            }
        }
        //return all classes that have the greatest number of classifications.
<span class="nc bnc" id="L1007" title="All 2 branches missed.">        if( centralTValue != 0){</span>
<span class="nc" id="L1008">            return centralTClasses;</span>
        } else {
<span class="nc" id="L1010">            return null;</span>
        }
        
    }
    
    /**
     * Returns description of the bias-variance decomposition results.
     *
     * @return the bias-variance decomposition results as a string
     */
    public String toString() {
        
<span class="nc" id="L1022">        String result = &quot;\nBias-Variance Decomposition Segmentation, Cross Validation\n&quot; +</span>
        &quot;with subsampling.\n&quot;;
        
<span class="nc bnc" id="L1025" title="All 2 branches missed.">        if (getClassifier() == null) {</span>
<span class="nc" id="L1026">            return &quot;Invalid setup&quot;;</span>
        }
        
<span class="nc" id="L1029">        result += &quot;\nClassifier    : &quot; + getClassifier().getClass().getName();</span>
<span class="nc bnc" id="L1030" title="All 2 branches missed.">        if (getClassifier() instanceof OptionHandler) {</span>
<span class="nc" id="L1031">            result += Utils.joinOptions(((OptionHandler)m_Classifier).getOptions());</span>
        }
<span class="nc" id="L1033">        result += &quot;\nData File     : &quot; + getDataFileName();</span>
<span class="nc" id="L1034">        result += &quot;\nClass Index   : &quot;;</span>
<span class="nc bnc" id="L1035" title="All 2 branches missed.">        if (getClassIndex() == 0) {</span>
<span class="nc" id="L1036">            result += &quot;last&quot;;</span>
        } else {
<span class="nc" id="L1038">            result += getClassIndex();</span>
        }
<span class="nc" id="L1040">        result += &quot;\nIterations    : &quot; + getClassifyIterations();</span>
<span class="nc" id="L1041">        result += &quot;\np             : &quot; + getP();</span>
<span class="nc" id="L1042">        result += &quot;\nTraining Size : &quot; + getTrainSize();</span>
<span class="nc" id="L1043">        result += &quot;\nSeed          : &quot; + getSeed();</span>
        
<span class="nc" id="L1045">        result += &quot;\n\nDefinition   : &quot; +&quot;Kohavi and Wolpert&quot;;</span>
<span class="nc" id="L1046">        result += &quot;\nError         :&quot; + Utils.doubleToString(getError(), 4);</span>
<span class="nc" id="L1047">        result += &quot;\nBias^2        :&quot; + Utils.doubleToString(getKWBias(), 4);</span>
<span class="nc" id="L1048">        result += &quot;\nVariance      :&quot; + Utils.doubleToString(getKWVariance(), 4);</span>
<span class="nc" id="L1049">        result += &quot;\nSigma^2       :&quot; + Utils.doubleToString(getKWSigma(), 4);</span>
        
<span class="nc" id="L1051">        result += &quot;\n\nDefinition   : &quot; +&quot;Webb&quot;;</span>
<span class="nc" id="L1052">        result += &quot;\nError         :&quot; + Utils.doubleToString(getError(), 4);</span>
<span class="nc" id="L1053">        result += &quot;\nBias          :&quot; + Utils.doubleToString(getWBias(), 4);</span>
<span class="nc" id="L1054">        result += &quot;\nVariance      :&quot; + Utils.doubleToString(getWVariance(), 4);</span>
        
<span class="nc" id="L1056">        return result;</span>
    }
    
    /**
     * Returns the revision string.
     * 
     * @return		the revision
     */
    public String getRevision() {
<span class="nc" id="L1065">      return RevisionUtils.extract(&quot;$Revision: 1.7 $&quot;);</span>
    }
    
    /**
     * Test method for this class
     *
     * @param args the command line arguments
     */
    public static void main(String [] args) {
        
        try {
<span class="nc" id="L1076">            BVDecomposeSegCVSub bvd = new BVDecomposeSegCVSub();</span>
            
            try {
<span class="nc" id="L1079">                bvd.setOptions(args);</span>
<span class="nc" id="L1080">                Utils.checkForRemainingOptions(args);</span>
<span class="nc" id="L1081">            } catch (Exception ex) {</span>
<span class="nc" id="L1082">                String result = ex.getMessage() + &quot;\nBVDecompose Options:\n\n&quot;;</span>
<span class="nc" id="L1083">                Enumeration enu = bvd.listOptions();</span>
<span class="nc bnc" id="L1084" title="All 2 branches missed.">                while (enu.hasMoreElements()) {</span>
<span class="nc" id="L1085">                    Option option = (Option) enu.nextElement();</span>
<span class="nc" id="L1086">                    result += option.synopsis() + &quot;\n&quot; + option.description() + &quot;\n&quot;;</span>
                }
<span class="nc" id="L1088">                throw new Exception(result);</span>
            }
            
<span class="nc" id="L1091">            bvd.decompose();</span>
            
<span class="nc" id="L1093">            System.out.println(bvd.toString());</span>
            
<span class="nc" id="L1095">        } catch (Exception ex) {</span>
<span class="nc" id="L1096">            System.err.println(ex.getMessage());</span>
        }
        
<span class="nc" id="L1099">    }</span>
    
    /**
     * Accepts an array of ints and randomises the values in the array, using the
     * random seed.
     *
     *@param index is the array of integers
     *@param random is the Random seed.
     */
    public final void randomize(int[] index, Random random) {
<span class="nc bnc" id="L1109" title="All 2 branches missed.">        for( int j = index.length - 1; j &gt; 0; j-- ){</span>
<span class="nc" id="L1110">            int k = random.nextInt( j + 1 );</span>
<span class="nc" id="L1111">            int temp = index[j];</span>
<span class="nc" id="L1112">            index[j] = index[k];</span>
<span class="nc" id="L1113">            index[k] = temp;</span>
        }
<span class="nc" id="L1115">    }</span>
}
</pre><div class="footer"><span class="right">Created with <a href="http://www.eclemma.org/jacoco">JaCoCo</a> 0.7.2.201409121644</span>AllTests (Nov 28, 2015 2:34:31 PM)</div></body></html>